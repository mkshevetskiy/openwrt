From 077f4adf2ca2e63159468b73985345500f4ef6d1 Mon Sep 17 00:00:00 2001
Message-ID: <077f4adf2ca2e63159468b73985345500f4ef6d1.1746795129.git.lorenzo@kernel.org>
From: Lorenzo Bianconi <lorenzo@kernel.org>
Date: Fri, 9 May 2025 10:55:05 +0200
Subject: [PATCH] mt76: npu: add wlan offload

Signed-off-by: Lorenzo Bianconi <lorenzo@kernel.org>
---
 Makefile             |   2 +-
 dma.c                | 121 ++++++----
 dma.h                |  67 ++++++
 mac80211.c           |  10 +-
 mt76.h               | 138 ++++++++++-
 mt7996/dma.c         |  92 ++++++--
 mt7996/init.c        |  81 +++++--
 mt7996/mac.c         | 150 +++++++++++-
 mt7996/main.c        |  20 +-
 mt7996/mcu.c         |   4 +-
 mt7996/mmio.c        |  34 ++-
 mt7996/mt7996.h      |  13 +-
 mt7996/mtk_debugfs.c |   9 +
 mt7996/pci.c         | 133 ++++++++++-
 mt7996/regs.h        |   5 +
 npu.c                | 546 +++++++++++++++++++++++++++++++++++++++++++
 tx.c                 |  19 +-
 17 files changed, 1296 insertions(+), 148 deletions(-)
 create mode 100644 npu.c

--- a/Makefile
+++ b/Makefile
@@ -12,7 +12,7 @@ obj-$(CONFIG_MT792x_USB) += mt792x-usb.o
 
 mt76-y := \
 	mmio.o util.o trace.o dma.o mac80211.o debugfs.o eeprom.o \
-	tx.o agg-rx.o mcu.o wed.o
+	tx.o agg-rx.o mcu.o wed.o npu.o
 
 mt76-$(CONFIG_PCI) += pci.o
 mt76-$(CONFIG_NL80211_TESTMODE) += testmode.o
--- a/dma.c
+++ b/dma.c
@@ -7,37 +7,6 @@
 #include "mt76.h"
 #include "dma.h"
 
-#if IS_ENABLED(CONFIG_NET_MEDIATEK_SOC_WED)
-
-#define Q_READ(_q, _field) ({						\
-	u32 _offset = offsetof(struct mt76_queue_regs, _field);		\
-	u32 _val;							\
-	if ((_q)->flags & MT_QFLAG_WED)					\
-		_val = mtk_wed_device_reg_read((_q)->wed,		\
-					       ((_q)->wed_regs +	\
-					        _offset));		\
-	else								\
-		_val = readl(&(_q)->regs->_field);			\
-	_val;								\
-})
-
-#define Q_WRITE(_q, _field, _val)	do {				\
-	u32 _offset = offsetof(struct mt76_queue_regs, _field);		\
-	if ((_q)->flags & MT_QFLAG_WED)					\
-		mtk_wed_device_reg_write((_q)->wed,			\
-					 ((_q)->wed_regs + _offset),	\
-					 _val);				\
-	else								\
-		writel(_val, &(_q)->regs->_field);			\
-} while (0)
-
-#else
-
-#define Q_READ(_q, _field)		readl(&(_q)->regs->_field)
-#define Q_WRITE(_q, _field, _val)	writel(_val, &(_q)->regs->_field)
-
-#endif
-
 static struct mt76_txwi_cache *
 mt76_alloc_txwi(struct mt76_dev *dev)
 {
@@ -189,10 +158,17 @@ static void
 mt76_dma_sync_idx(struct mt76_dev *dev, struct mt76_queue *q)
 {
 	Q_WRITE(q, desc_base, q->desc_dma);
+#if MAGIC_ENABLE	
 	if (q->flags & MT_QFLAG_WED_RRO_EN)
 		Q_WRITE(q, ring_size, MT_DMA_RRO_EN | q->ndesc);
 	else
+#endif		
 		Q_WRITE(q, ring_size, q->ndesc);
+
+	if (mt76_queue_is_npu_tx(q)) {
+		writel(q->desc_dma, &q->regs->desc_base);
+		writel(q->ndesc, &q->regs->ring_size);
+	}
 	q->head = Q_READ(q, dma_idx);
 	q->tail = q->head;
 }
@@ -203,12 +179,14 @@ void __mt76_dma_queue_reset(struct mt76_
 	if (!q || !q->ndesc)
 		return;
 
-	if (!mt76_queue_is_wed_rro_ind(q)) {
+	if (!mt76_queue_is_npu(q) && !mt76_queue_is_wed_rro_ind(q) &&
+	    !mt76_queue_is_wed_rro_rxdmad_c(q)) {   //rro3.1
+		struct mt76_desc *desc = q->desc;
 		int i;
 
 		/* clear descriptors */
 		for (i = 0; i < q->ndesc; i++)
-			q->desc[i].ctrl = cpu_to_le32(MT_DMA_CTL_DMA_DONE);
+			desc[i].ctrl = cpu_to_le32(MT_DMA_CTL_DMA_DONE);
 	}
 
 	if (reset_idx) {
@@ -228,8 +206,8 @@ mt76_dma_add_rx_buf(struct mt76_dev *dev
 		    struct mt76_queue_buf *buf, void *data,
 		    struct mt76_rxwi_cache *rxwi)
 {
+	struct mt76_desc *desc = q->desc + q->head * sizeof(*desc);
 	struct mt76_queue_entry *entry = &q->entry[q->head];
-	struct mt76_desc *desc;
 	int idx = q->head;
 	u32 buf1 = 0, ctrl, info = 0;
 	int rx_token;
@@ -241,8 +219,12 @@ mt76_dma_add_rx_buf(struct mt76_dev *dev
 		data = &rro_desc[q->head];
 		goto done;
 	}
+	  /*rro3.1*/
+	if (mt76_queue_is_wed_rro_rxdmad_c(q)) {
+		data = desc;
+		goto done;
+	}
 
-	desc = &q->desc[q->head];
 	ctrl = FIELD_PREP(MT_DMA_CTL_SD_LEN0, buf[0].len);
 #ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT
 	buf1 = FIELD_PREP(MT_DMA_CTL_SDP0_H, buf->addr >> 32);
@@ -273,13 +255,13 @@ mt76_dma_add_rx_buf(struct mt76_dev *dev
 		if (dev->drv->rx_rro_fill_msdu_pg(dev, q, buf->addr, data))
 			return	-ENOMEM;
 	}
-
+#if MAGIC_ENABLE
 	if (q->flags & MT_QFLAG_WED_RRO_EN) {
 		info |= FIELD_PREP(MT_DMA_MAGIC_MASK, q->magic_cnt);
 		if ((q->head + 1) == q->ndesc)
 			q->magic_cnt = (q->magic_cnt + 1) % MT_DMA_MAGIC_CNT;
 	}
-
+#endif
 	WRITE_ONCE(desc->buf0, cpu_to_le32(buf->addr));
 	WRITE_ONCE(desc->buf1, cpu_to_le32(buf1));
 	WRITE_ONCE(desc->ctrl, cpu_to_le32(ctrl));
@@ -319,7 +301,7 @@ mt76_dma_add_buf(struct mt76_dev *dev, s
 		idx = q->head;
 		next = (q->head + 1) % q->ndesc;
 
-		desc = &q->desc[idx];
+		desc = q->desc + idx * sizeof(*desc);
 		entry = &q->entry[idx];
 
 		if (buf[0].skip_unmap)
@@ -412,6 +394,7 @@ mt76_dma_tx_cleanup(struct mt76_dev *dev
 
 	while (q->queued > 0 && q->tail != last) {
 		mt76_dma_tx_cleanup_idx(dev, q, q->tail, &entry);
+		mt76_npu_txdesc_cleanup(q, q->tail);
 		mt76_queue_tx_complete(dev, q, &entry);
 
 		if (entry.txwi) {
@@ -439,8 +422,8 @@ static void *
 mt76_dma_get_buf(struct mt76_dev *dev, struct mt76_queue *q, int idx,
 		 int *len, u32 *info, bool *more, bool *drop, bool flush)
 {
+	struct mt76_desc *desc = q->desc + idx * sizeof(*desc);
 	struct mt76_queue_entry *e = &q->entry[idx];
-	struct mt76_desc *desc = &q->desc[idx];
 	u32 ctrl, desc_info, buf1;
 	void *buf = e->buf;
 	int reason;
@@ -591,10 +574,24 @@ mt76_dma_dequeue(struct mt76_dev *dev, s
 
 		if (q->tail == q->ndesc - 1)
 			q->magic_cnt = (q->magic_cnt + 1) % MT_DMA_WED_IND_CMD_CNT;
+	} else if (mt76_queue_is_wed_rro_rxdmad_c(q)) {   //rro3.1
+		struct mt76_rro_rxdmad_c *dmad;
+
+		if (flush)
+			goto done;
+
+		dmad = q->entry[idx].buf;
+		if (dmad->magic_cnt != q->magic_cnt)
+			return NULL;
+
+		if (q->tail == q->ndesc - 1)
+			q->magic_cnt = (q->magic_cnt + 1) % MT_DMA_MAGIC_CNT;		
 	} else {
+		struct mt76_desc *desc = q->desc;
+
 		if (flush)
-			q->desc[idx].ctrl |= cpu_to_le32(MT_DMA_CTL_DMA_DONE);
-		else if (!(q->desc[idx].ctrl & cpu_to_le32(MT_DMA_CTL_DMA_DONE)))
+			desc[idx].ctrl |= cpu_to_le32(MT_DMA_CTL_DMA_DONE);
+		else if (!(desc[idx].ctrl & cpu_to_le32(MT_DMA_CTL_DMA_DONE)))
 			return NULL;
 	}
 
@@ -660,6 +657,10 @@ mt76_dma_tx_queue_skb(struct mt76_phy *p
 	dma_addr_t addr;
 	u8 *txwi;
 
+	/* FIXME: Take into account unlinear skbs */
+	if (mt76_npu_device_active(dev) && skb_linearize(skb))
+		goto free_skb;
+
 	if (test_bit(MT76_RESET, &phy->state)) {
 		phy->tx_dbg_stats.tx_drop[MT_TX_DROP_RESET_STATE]++;
 		goto free_skb;
@@ -722,6 +723,9 @@ mt76_dma_tx_queue_skb(struct mt76_phy *p
 		goto unmap;
 
 	phy->tx_dbg_stats.tx_to_hw++;
+	if (mt76_npu_device_active(dev))
+		return mt76_npu_dma_add_buf(phy, q, skb, &tx_info.buf[1], txwi);
+
 	return mt76_dma_add_buf(dev, q, tx_info.buf, tx_info.nbuf,
 				tx_info.info, tx_info.skb, t);
 
@@ -769,7 +773,8 @@ int mt76_dma_rx_fill(struct mt76_dev *de
 		struct mt76_queue_buf qbuf = {};
 		void *buf = NULL;
 
-		if (mt76_queue_is_wed_rro_ind(q))
+		/*rro3.1*/
+		if (mt76_queue_is_wed_rro_ind(q) || mt76_queue_is_wed_rro_rxdmad_c(q))
 			goto done;
 
 		buf = page_frag_alloc(&q->rx_page, q->buf_size, GFP_ATOMIC | GFP_DMA32);
@@ -817,9 +822,17 @@ mt76_dma_alloc_queue(struct mt76_dev *de
 	q->ndesc = n_desc;
 	q->buf_size = bufsize;
 	q->hw_idx = idx;
+	q->dev = dev;
+
+	if (mt76_queue_is_wed_rro_ind(q))
+		size = sizeof(struct mt76_wed_rro_desc);
+	else if (mt76_queue_is_npu_tx(q))
+		size = sizeof(struct airoha_npu_tx_dma_desc);
+	else if (mt76_queue_is_npu_rx(q))
+		size = sizeof(struct airoha_npu_rx_dma_desc);
+	else
+		size = sizeof(struct mt76_desc);
 
-	size = mt76_queue_is_wed_rro_ind(q) ? sizeof(struct mt76_wed_rro_desc)
-					    : sizeof(struct mt76_desc);
 	q->desc = dmam_alloc_coherent(dev->dma_dev, q->ndesc * size,
 				      &q->desc_dma, GFP_KERNEL);
 	if (!q->desc)
@@ -838,11 +851,16 @@ mt76_dma_alloc_queue(struct mt76_dev *de
 		}
 	}
 
+	/*rro3.1*/
+	if (mt76_queue_is_wed_rro_rxdmad_c(q) && dev->drv->rx_init_rxdmad_c)
+		dev->drv->rx_init_rxdmad_c(dev, q);
+
 	size = q->ndesc * sizeof(*q->entry);
 	q->entry = devm_kzalloc(dev->dev, size, GFP_KERNEL);
 	if (!q->entry)
 		return -ENOMEM;
 
+	mt76_npu_queue_setup(dev, q);
 	ret = mt76_wed_dma_setup(dev, q, false);
 	if (ret)
 		return ret;
@@ -907,11 +925,12 @@ mt76_dma_rx_reset(struct mt76_dev *dev,
 	if (!q->ndesc)
 		return;
 
-	if (!mt76_queue_is_wed_rro_ind(q)) {
+	if (!mt76_queue_is_wed_rro_ind(q) && !mt76_queue_is_wed_rro_rxdmad_c(q)) {
+		struct mt76_desc *desc = q->desc;
 		int i;
 
 		for (i = 0; i < q->ndesc; i++)
-			q->desc[i].ctrl = cpu_to_le32(MT_DMA_CTL_DMA_DONE);
+			desc[i].ctrl = cpu_to_le32(MT_DMA_CTL_DMA_DONE);
 	}
 
 	mt76_dma_rx_cleanup(dev, q);
@@ -970,14 +989,14 @@ mt76_dma_rx_process(struct mt76_dev *dev
 	bool check_ddone = false;
 	bool allow_direct = !mt76_queue_is_wed_rx(q);
 	bool more;
-
+#if MAGIC_ENABLE
 	if ((q->flags & MT_QFLAG_WED_RRO_EN) ||
 	    (IS_ENABLED(CONFIG_NET_MEDIATEK_SOC_WED) &&
 	    mt76_queue_is_wed_tx_free(q))) {
 		dma_idx = Q_READ(q, dma_idx);
 		check_ddone = true;
 	}
-
+#endif
 	while (done < budget) {
 		bool drop = false;
 		u32 info;
@@ -998,6 +1017,11 @@ mt76_dma_rx_process(struct mt76_dev *dev
 		if (mt76_queue_is_wed_rro_ind(q) && dev->drv->rx_rro_ind_process)
 			dev->drv->rx_rro_ind_process(dev, data);
 
+		/*rro3.1*/
+		if(mt76_queue_is_wed_rro_rxdmad_c(q) && dev->drv->rx_rro_rxdmadc_process)
+			dev->drv->rx_rro_rxdmadc_process(dev, data);
+
+
 		if (mt76_queue_is_wed_rro(q)) {
 			done++;
 			continue;
@@ -1080,6 +1104,7 @@ int mt76_dma_rx_poll(struct napi_struct
 
 	return done;
 }
+
 EXPORT_SYMBOL_GPL(mt76_dma_rx_poll);
 
 static int
--- a/dma.h
+++ b/dma.h
@@ -45,6 +45,73 @@
 #define MT_FCE_INFO_LEN			4
 #define MT_RX_RXWI_LEN			32
 
+#if IS_ENABLED(CONFIG_NET_MEDIATEK_SOC_WED)
+
+#define Q_READ(_q, _field) ({						\
+	u32 _offset = offsetof(struct mt76_queue_regs, _field);		\
+	u32 _val;							\
+	if ((_q)->flags & MT_QFLAG_WED)					\
+		_val = mtk_wed_device_reg_read((_q)->wed,		\
+					       ((_q)->wed_regs +	\
+					        _offset));		\
+	else								\
+		_val = readl(&(_q)->regs->_field);			\
+	_val;								\
+})
+
+#define Q_WRITE(_q, _field, _val)	do {				\
+	u32 _offset = offsetof(struct mt76_queue_regs, _field);		\
+	if ((_q)->flags & MT_QFLAG_WED)					\
+		mtk_wed_device_reg_write((_q)->wed,			\
+					 ((_q)->wed_regs + _offset),	\
+					 _val);				\
+	else								\
+		writel(_val, &(_q)->regs->_field);			\
+} while (0)
+
+#elif (IS_BUILTIN(CONFIG_NET_AIROHA_NPU) || IS_MODULE(CONFIG_NET_AIROHA_NPU))
+
+#define Q_READ(_q, _field) ({						\
+	u32 _offset = offsetof(struct mt76_queue_regs, _field);		\
+	u32 _val;							\
+	if ((_q)->flags & MT_QFLAG_NPU) {				\
+		struct airoha_npu *npu;					\
+									\
+		rcu_read_lock();					\
+		npu = rcu_dereference(q->dev->mmio.npu);		\
+		if (npu)						\
+			regmap_read(npu->regmap,			\
+				    ((_q)->wed_regs + _offset), &_val);	\
+		rcu_read_unlock();					\
+	} else {							\
+		_val = readl(&(_q)->regs->_field);			\
+	}								\
+	_val;								\
+})
+
+#define Q_WRITE(_q, _field, _val)	do {				\
+	u32 _offset = offsetof(struct mt76_queue_regs, _field);		\
+	if ((_q)->flags & MT_QFLAG_NPU) {				\
+		struct airoha_npu *npu;					\
+									\
+		rcu_read_lock();					\
+		npu = rcu_dereference(q->dev->mmio.npu);		\
+		if (npu)						\
+			regmap_write(npu->regmap,			\
+				     ((_q)->wed_regs + _offset), _val);	\
+		rcu_read_unlock();					\
+	} else {							\
+		writel(_val, &(_q)->regs->_field);			\
+	}								\
+} while (0)
+
+#else
+
+#define Q_READ(_q, _field)		readl(&(_q)->regs->_field)
+#define Q_WRITE(_q, _field, _val)	writel(_val, &(_q)->regs->_field)
+
+#endif
+
 struct mt76_desc {
 	__le32 buf0;
 	__le32 ctrl;
--- a/mac80211.c
+++ b/mac80211.c
@@ -755,6 +755,7 @@ void mt76_free_device(struct mt76_dev *d
 		destroy_workqueue(dev->wq);
 		dev->wq = NULL;
 	}
+	mt76_npu_deinit(dev);
 	ieee80211_free_hw(dev->hw);
 }
 EXPORT_SYMBOL_GPL(mt76_free_device);
@@ -798,7 +799,9 @@ static void mt76_rx_release_amsdu(struct
 	}
 
 	if (mt76_queue_is_wed_rro_data(rxq))
-		q = MT_RXQ_RRO_IND;
+		/*rro3.1*/
+		q = (dev->hwrro_mode == MT76_HWRRO_V3) ?
+		    MT_RXQ_RRO_IND : MT_RXQ_RRO_RXDMAD_C;
 
 	__skb_queue_tail(&dev->rx_skb[q], skb);
 }
@@ -1458,11 +1461,10 @@ void mt76_rx_poll_complete(struct mt76_d
 
 	while ((skb = __skb_dequeue(&dev->rx_skb[q])) != NULL) {
 		mt76_check_sta(dev, skb);
-		#ifdef CONFIG_NET_MEDIATEK_SOC_WED	
-		if (mtk_wed_device_active(&dev->mmio.wed))
+		if (mtk_wed_device_active(&dev->mmio.wed) ||
+		    mt76_npu_device_active(dev))
 			__skb_queue_tail(&frames, skb);
 		else
-		#endif
 			mt76_rx_aggr_reorder(dev, skb, &frames);
 	}
 
--- a/mt76.h
+++ b/mt76.h
@@ -13,6 +13,8 @@
 #include <linux/leds.h>
 #include <linux/usb.h>
 #include <linux/average.h>
+#include <linux/pci.h>
+#include <linux/soc/airoha/airoha_offload.h>
 #include <linux/soc/mediatek/mtk_wed.h>
 #include <net/netlink.h>
 #include <net/mac80211.h>
@@ -23,6 +25,7 @@
 #endif
 #include "util.h"
 #include "testmode.h"
+#define MAGIC_ENABLE 0
 
 #define CHAN2G(_idx, _freq) {			\
 	.band = NL80211_BAND_2GHZ,		\
@@ -61,6 +64,7 @@
 #define MT_QFLAG_WED		BIT(5)
 #define MT_QFLAG_WED_RRO	BIT(6)
 #define MT_QFLAG_WED_RRO_EN	BIT(7)
+#define MT_QFLAG_NPU		BIT(8)
 
 #define __MT_WED_Q(_type, _n)	(MT_QFLAG_WED | \
 				 FIELD_PREP(MT_QFLAG_WED_TYPE, _type) | \
@@ -73,6 +77,14 @@
 #define MT_WED_RRO_Q_DATA(_n)	__MT_WED_RRO_Q(MT76_WED_RRO_Q_DATA, _n)
 #define MT_WED_RRO_Q_MSDU_PG(_n)	__MT_WED_RRO_Q(MT76_WED_RRO_Q_MSDU_PG, _n)
 #define MT_WED_RRO_Q_IND	__MT_WED_RRO_Q(MT76_WED_RRO_Q_IND, 0)
+/*rro3.1*/
+#define MT_WED_RRO_Q_RXDMAD_C	__MT_WED_RRO_Q(MT76_WED_RRO_Q_RXDMAD_C, 0)
+
+#define __MT_NPU_Q(_type, _n)	(MT_QFLAG_NPU | \
+				 FIELD_PREP(MT_QFLAG_WED_TYPE, _type) | \
+				 FIELD_PREP(MT_QFLAG_WED_RING, _n))
+#define MT_NPU_Q_TX(_n)		__MT_NPU_Q(MT76_WED_Q_TX, _n)
+#define MT_NPU_Q_RX(_n)		__MT_NPU_Q(MT76_WED_Q_RX, _n)
 
 #define AMPDU_ADDBA_SUCC_SHFT IEEE80211_NUM_TIDS
 
@@ -99,6 +111,15 @@ enum mt76_wed_type {
 	MT76_WED_RRO_Q_DATA,
 	MT76_WED_RRO_Q_MSDU_PG,
 	MT76_WED_RRO_Q_IND,
+	/*rro3.1*/
+	MT76_WED_RRO_Q_RXDMAD_C,
+};
+
+/*rro3.1*/
+enum mt76_hwrro_mode{
+       MT76_HWRRO_DISABLE,
+       MT76_HWRRO_V3,
+       MT76_HWRRO_V3_1,	
 };
 
 struct mt76_bus_ops {
@@ -157,6 +178,10 @@ enum mt76_rxq_id {
 	MT_RXQ_TXFREE_BAND1,
 	MT_RXQ_TXFREE_BAND2,
 	MT_RXQ_RRO_IND,
+	/*rro3.1*/
+	MT_RXQ_RRO_RXDMAD_C,	
+	MT_RXQ_NPU0,
+	MT_RXQ_NPU1,
 	__MT_RXQ_MAX
 };
 
@@ -264,7 +289,7 @@ struct mt76_queue {
 	spinlock_t cleanup_lock;
 	struct mt76_queue_entry *entry;
 	struct mt76_rro_desc *rro_desc;
-	struct mt76_desc *desc;
+	void *desc;
 
 	u16 first;
 	u16 head;
@@ -282,6 +307,7 @@ struct mt76_queue {
 	u8 magic_cnt;
 
 	struct mtk_wed_device *wed;
+	struct mt76_dev *dev;
 	u32 wed_regs;
 
 	dma_addr_t desc_dma;
@@ -472,6 +498,42 @@ struct mt76_wed_rro_ind {
 	u32 magic_cnt	: 3;
 };
 
+/*rro3.1*/
+struct mt76_rro_rxdmad_c {
+	u32 sdp0_31_0;
+	u32 header_ofst     :7;
+	u32 ver             :1;
+	u32 to_host         :1;
+	u32 ring_info       :2;
+	u32 dst_sel         :2;
+	u32 pn_chk_fail     :1;
+	u32 rsv             :2;
+	u32 sdl0            :14;
+	u32 ls              :1;
+	u32 rsv2            :1;
+	u32 sdp0_35_32      :4;
+	u32 rsv3            :2;
+	u32 sca_gat         :1;
+	u32 par_se          :1;
+	u32 rss_hash        :4;
+	u32 ind_reason      :4;
+	u32 rx_token_id     :16;
+	u32 cs_status       :4;
+	u32 cs_type         :4;
+	u32 c               :1;
+	u32 f               :1;
+	u32 un              :1;
+	u32 is_fc_data      :1;
+	u32 uc              :1;
+	u32 mc              :1;
+	u32 bc              :1;
+	u32 rsv4            :1;
+	u32 wcid            :12;
+	u32 magic_cnt       :4;
+};
+
+
+
 struct mt76_txwi_cache {
 	struct list_head list;
 	dma_addr_t dma_addr;
@@ -607,6 +669,11 @@ struct mt76_driver_ops {
 	int (*rx_rro_fill_msdu_pg)(struct mt76_dev *dev, struct mt76_queue *q,
 				   dma_addr_t p, void *data);
 
+	/*rro3.1*/
+	void (*rx_init_rxdmad_c)(struct mt76_dev *dev, struct mt76_queue *q);
+	void (*rx_rro_rxdmadc_process)(struct mt76_dev *mdev, void *data);
+
+
 	void (*rx_poll_complete)(struct mt76_dev *dev, enum mt76_rxq_id q);
 
 	void (*sta_ps)(struct mt76_dev *dev, struct ieee80211_sta *sta,
@@ -749,6 +816,9 @@ struct mt76_mmio {
 	struct mtk_wed_device wed_hif2;
 	struct completion wed_reset;
 	struct completion wed_reset_complete;
+
+	struct airoha_ppe_dev __rcu *ppe_dev;
+	struct airoha_npu __rcu *npu;
 };
 
 struct mt76_rx_status {
@@ -1124,6 +1194,8 @@ struct mt76_dev {
 	bool mgmt_pwr_enhance;
 	bool lpi_mode;
 	s8 **afc_power_table;
+	/*rro3.1*/
+	enum mt76_hwrro_mode hwrro_mode;
 
 #ifdef CONFIG_NL80211_TESTMODE
 	const struct mt76_testmode_ops *test_ops;
@@ -1705,6 +1777,27 @@ int mt76_testmode_dump(struct ieee80211_
 int mt76_testmode_set_state(struct mt76_phy *phy, enum mt76_testmode_state state);
 int mt76_testmode_alloc_skb(struct mt76_phy *phy, u32 len);
 
+int mt76_npu_rx_poll(struct napi_struct *napi, int budget);
+int mt76_npu_rx_queue_init(struct mt76_dev *dev, struct mt76_queue *q);
+int mt76_npu_dma_add_buf(struct mt76_phy *phy, struct mt76_queue *q,
+			 struct sk_buff *skb, struct mt76_queue_buf *buf,
+			 void *txwi_ptr);
+int mt76_npu_init_rxd(struct mt76_dev *dev, struct airoha_npu *npu);
+int mt76_npu_init_txd(struct mt76_dev *dev, struct airoha_npu *npu, int size);
+void mt76_npu_init_tx_done(struct mt76_dev *dev, struct airoha_npu *npu);
+int mt76_npu_init(struct mt76_dev *dev);
+void mt76_npu_deinit(struct mt76_dev *dev);
+void mt76_npu_queue_setup(struct mt76_dev *dev, struct mt76_queue *q);
+void mt76_npu_txdesc_cleanup(struct mt76_queue *q, int index);
+int mt76_npu_net_setup_tc(struct ieee80211_hw *hw, struct ieee80211_vif *vif,
+			  struct net_device *dev, enum tc_setup_type type,
+			  void *type_data);
+
+static inline bool mt76_npu_device_active(struct mt76_dev *dev)
+{
+	return !!rcu_access_pointer(dev->mmio.npu);
+}
+
 static inline void
 mt76_testmode_param_set(struct mt76_testmode_data *td, u16 idx)
 {
@@ -1925,6 +2018,13 @@ static inline bool mt76_queue_is_wed_rro
 	       FIELD_GET(MT_QFLAG_WED_TYPE, q->flags) == MT76_WED_RRO_Q_IND;
 }
 
+/*rro3.1*/
+static inline bool mt76_queue_is_wed_rro_rxdmad_c(struct mt76_queue *q)
+{
+	return mt76_queue_is_wed_rro(q) &&
+	       FIELD_GET(MT_QFLAG_WED_TYPE, q->flags) == MT76_WED_RRO_Q_RXDMAD_C;
+}
+
 static inline bool mt76_queue_is_wed_rro_data(struct mt76_queue *q)
 {
 	return mt76_queue_is_wed_rro(q) &&
@@ -1943,10 +2043,42 @@ static inline bool mt76_queue_is_wed_rx(
 	       FIELD_GET(MT_QFLAG_WED_TYPE, q->flags) == MT76_WED_Q_RX;
 }
 
+static inline bool mt76_queue_is_npu(struct mt76_queue *q)
+{
+	return q->flags & MT_QFLAG_NPU;
+}
+
+static inline bool mt76_queue_is_npu_tx(struct mt76_queue *q)
+{
+	return mt76_queue_is_npu(q) &&
+	       FIELD_GET(MT_QFLAG_WED_TYPE, q->flags) == MT76_WED_Q_TX;
+}
+
+static inline bool mt76_queue_is_npu_rx(struct mt76_queue *q)
+{
+	return mt76_queue_is_npu(q) &&
+	       FIELD_GET(MT_QFLAG_WED_TYPE, q->flags) == MT76_WED_Q_RX;
+}
+
 struct mt76_txwi_cache *
 mt76_token_release(struct mt76_dev *dev, int token, bool *wake);
-int mt76_token_consume(struct mt76_dev *dev, struct mt76_txwi_cache **ptxwi,
-		       u8 phy_idx);
+int __mt76_token_consume(struct mt76_dev *dev, struct mt76_txwi_cache **ptxwi,
+			 u8 phy_idx, int token_start);
+
+static inline int mt76_token_consume(struct mt76_dev *dev,
+				     struct mt76_txwi_cache **ptxwi,
+				     u8 phy_idx)
+{
+	int token_start = 0;
+
+#ifdef CONFIG_NET_MEDIATEK_SOC_WED
+	if (mtk_wed_device_active(&dev->mmio.wed))
+		token_start = dev->mmio.wed.wlan.nbuf;
+#endif
+
+	return __mt76_token_consume(dev, ptxwi, phy_idx, token_start);
+}
+
 void __mt76_set_tx_blocked(struct mt76_dev *dev, bool blocked);
 struct mt76_rxwi_cache *mt76_rx_token_release(struct mt76_dev *dev, int token);
 int mt76_rx_token_consume(struct mt76_dev *dev, void *ptr,
--- a/mt7996/dma.c
+++ b/mt7996/dma.c
@@ -31,6 +31,10 @@ int mt7996_init_tx_queues(struct mt7996_
 			flags = MT_WED_Q_TX(idx);
 	}
 #endif
+
+	if (mt76_npu_device_active(&dev->mt76))
+		flags = MT_NPU_Q_TX(phy->mt76->band_idx);
+
 	return mt76_connac_init_tx_queues(phy->mt76, idx, n_desc,
 					  ring_base, wed, flags);
 }
@@ -90,12 +94,14 @@ static void mt7996_dma_config(struct mt7
 		break;
 	}
 
-	if (dev->has_rro) {
+	if (dev->mt76.hwrro_mode) {
 		/* band0 */
 		RXQ_CONFIG(MT_RXQ_RRO_BAND0, WFDMA0, MT_INT_RX_DONE_RRO_BAND0,
 			   MT7996_RXQ_RRO_BAND0);
-		RXQ_CONFIG(MT_RXQ_MSDU_PAGE_BAND0, WFDMA0, MT_INT_RX_DONE_MSDU_PG_BAND0,
-			   MT7996_RXQ_MSDU_PG_BAND0);
+		if (dev->mt76.hwrro_mode == MT76_HWRRO_V3)
+			RXQ_CONFIG(MT_RXQ_MSDU_PAGE_BAND0, WFDMA0, MT_INT_RX_DONE_MSDU_PG_BAND0,
+					   MT7996_RXQ_MSDU_PG_BAND0);
+
 		if (is_mt7996(&dev->mt76)) {
 			RXQ_CONFIG(MT_RXQ_TXFREE_BAND0, WFDMA0, MT_INT_RX_TXFREE_MAIN,
 				   MT7996_RXQ_TXFREE0);
@@ -114,8 +120,13 @@ static void mt7996_dma_config(struct mt7
 				   MT7996_RXQ_RRO_BAND1);
 		}
 
-		RXQ_CONFIG(MT_RXQ_RRO_IND, WFDMA0, MT_INT_RX_DONE_RRO_IND,
-			   MT7996_RXQ_RRO_IND);
+		if (dev->mt76.hwrro_mode == MT76_HWRRO_V3)
+			RXQ_CONFIG(MT_RXQ_RRO_IND, WFDMA0, MT_INT_RX_DONE_RRO_IND,
+				   MT7996_RXQ_RRO_IND);
+		else
+			RXQ_CONFIG(MT_RXQ_RRO_RXDMAD_C, WFDMA0, MT_INT_RX_DONE_RRO_RXDMAD_C,
+				   MT7996_RXQ_RRO_RXDMAD_C);
+
 	}
 
 	/* data tx queue */
@@ -211,7 +222,7 @@ static void __mt7996_dma_prefetch(struct
 	mt76_wr(dev, MT_RXQ_EXT_CTRL(queue) + ofs, PREFETCH(0x10));
 
 	/* Rx RRO Rings */
-	if (dev->has_rro) {
+	if (dev->mt76.hwrro_mode) {
 		mt76_wr(dev, MT_RXQ_EXT_CTRL(MT_RXQ_RRO_BAND0) + ofs, PREFETCH(0x10));
 		queue = is_mt7996(&dev->mt76) ? MT_RXQ_RRO_BAND2 : MT_RXQ_RRO_BAND1;
 		mt76_wr(dev, MT_RXQ_EXT_CTRL(queue) + ofs, PREFETCH(0x10));
@@ -341,7 +352,7 @@ void mt7996_dma_start(struct mt7996_dev
 		mtk_wed_device_start(wed, wed_irq_mask);
 	}
 #endif
-	if (!mt7996_has_wa(dev))
+	if (!mt7996_has_wa(dev) || mt76_npu_device_active(&dev->mt76))
 		irq_mask &= ~(MT_INT_RX(MT_RXQ_MAIN_WA) | MT_INT_RX(MT_RXQ_BAND1_WA));
 
 	irq_mask = reset ? MT_INT_MCU_CMD : irq_mask;
@@ -484,7 +495,7 @@ static void mt7996_dma_enable(struct mt7
 		 */
 #ifdef CONFIG_NET_MEDIATEK_SOC_WED	
 		if (mtk_wed_device_active(&dev->mt76.mmio.wed) &&
-		    dev->has_rro) {
+		    dev->mt76.hwrro_mode) {
 			u32 intr = is_mt7996(&dev->mt76) ?
 				   MT_WFDMA0_RX_INT_SEL_RING6 :
 				   MT_WFDMA0_RX_INT_SEL_RING9 |
@@ -510,6 +521,26 @@ int mt7996_dma_rro_init(struct mt7996_de
 #endif
 	int ret;
 
+	if (dev->mt76.hwrro_mode == MT76_HWRRO_V3_1) {
+		/* rxdmad_c */
+		mdev->q_rx[MT_RXQ_RRO_RXDMAD_C].flags = MT_WED_RRO_Q_RXDMAD_C;
+#ifdef CONFIG_NET_MEDIATEK_SOC_WED		
+		if (mtk_wed_device_active(&mdev->mmio.wed) &&
+			mtk_wed_get_rx_capa(&mdev->mmio.wed))
+			mdev->q_rx[MT_RXQ_RRO_RXDMAD_C].wed = &mdev->mmio.wed;
+#endif	
+		ret = mt76_queue_alloc(dev, &mdev->q_rx[MT_RXQ_RRO_RXDMAD_C],
+						   MT_RXQ_ID(MT_RXQ_RRO_RXDMAD_C),
+						   MT7996_RX_RING_SIZE,
+					   MT7996_RX_BUF_SIZE,
+					   MT_RXQ_RRO_AP_RING_BASE);
+		if (ret)
+			return ret;
+	
+		goto start_hw_rro;
+	}
+
+
 	/* ind cmd */
 	mdev->q_rx[MT_RXQ_RRO_IND].flags = MT_WED_RRO_Q_IND;
 #ifdef CONFIG_NET_MEDIATEK_SOC_WED		
@@ -576,7 +607,7 @@ int mt7996_dma_rro_init(struct mt7996_de
 			return ret;
 	}
 
-
+start_hw_rro:
 #ifdef CONFIG_NET_MEDIATEK_SOC_WED	
 	if (mtk_wed_device_active(&mdev->mmio.wed)) {
 		irq_mask = mdev->mmio.irqmask |
@@ -589,19 +620,31 @@ int mt7996_dma_rro_init(struct mt7996_de
 #endif	
 	{
 		if (is_mt7996(&dev->mt76)) {
-			mt76_queue_rx_init(dev, MT_RXQ_TXFREE_BAND0, mt76_dma_rx_poll);
-			mt76_queue_rx_init(dev, MT_RXQ_TXFREE_BAND2, mt76_dma_rx_poll);
 			mt76_queue_rx_init(dev, MT_RXQ_MSDU_PAGE_BAND1, mt76_dma_rx_poll);
 			mt76_queue_rx_init(dev, MT_RXQ_MSDU_PAGE_BAND2, mt76_dma_rx_poll);
 		}
-		mt76_queue_rx_init(dev, MT_RXQ_RRO_BAND0, mt76_dma_rx_poll);
-		mt76_queue_rx_init(dev, MT_RXQ_RRO_BAND1, mt76_dma_rx_poll);
-		if (mt7996_band_valid(dev, MT_BAND2))
-			mt76_queue_rx_init(dev, MT_RXQ_RRO_BAND2, mt76_dma_rx_poll);
-		mt76_queue_rx_init(dev, MT_RXQ_RRO_IND, mt76_dma_rx_poll);
-		mt76_queue_rx_init(dev, MT_RXQ_MSDU_PAGE_BAND0, mt76_dma_rx_poll);
 
-		mt7996_irq_enable(dev, MT_INT_RRO_RX_DONE);
+		if (mt76_npu_device_active(&dev->mt76)) {
+			ret = mt76_npu_rx_queue_init(&dev->mt76,
+						     &mdev->q_rx[MT_RXQ_NPU0]);
+			if (ret)
+				return ret;
+
+			ret = mt76_npu_rx_queue_init(&dev->mt76,
+						     &mdev->q_rx[MT_RXQ_NPU1]);
+			if (ret)
+				return ret;
+		}
+
+		if (dev->mt76.hwrro_mode == MT76_HWRRO_V3_1) {
+			mt76_queue_rx_init(dev, MT_RXQ_RRO_RXDMAD_C, mt76_dma_rx_poll);
+		} else {
+			mt76_queue_rx_init(dev, MT_RXQ_RRO_IND, mt76_dma_rx_poll);
+			mt76_queue_rx_init(dev, MT_RXQ_MSDU_PAGE_BAND0, mt76_dma_rx_poll);
+		}
+
+		if (!mt76_npu_device_active(&dev->mt76))
+			mt7996_irq_enable(dev, MT_INT_RRO_RX_DONE);
 	}
 
 	return 0;
@@ -697,7 +740,7 @@ int mt7996_dma_init(struct mt7996_dev *d
 	/* tx free notify event from WA for band0 */
 	#ifdef CONFIG_NET_MEDIATEK_SOC_WED	
 	if (mtk_wed_device_active(wed) &&
-	    ((is_mt7996(&dev->mt76) && !dev->has_rro) ||
+	    ((is_mt7996(&dev->mt76) && !dev->mt76.hwrro_mode) ||
 	     (is_mt7992(&dev->mt76)))) {
 		dev->mt76.q_rx[MT_RXQ_MAIN_WA].flags = MT_WED_Q_TXFREE;
 		dev->mt76.q_rx[MT_RXQ_MAIN_WA].wed = wed;
@@ -766,7 +809,7 @@ int mt7996_dma_init(struct mt7996_dev *d
 		 * use pcie0's rx ring3, but, redirect pcie0 rx ring3 interrupt to pcie1
 		 */
 		#ifdef CONFIG_NET_MEDIATEK_SOC_WED	
-		if (mtk_wed_device_active(wed_hif2) && !dev->has_rro) {
+		if (mtk_wed_device_active(wed_hif2) && !dev->mt76.hwrro_mode) {
 			dev->mt76.q_rx[MT_RXQ_BAND2_WA].flags = MT_WED_Q_TXFREE;
 			dev->mt76.q_rx[MT_RXQ_BAND2_WA].wed = wed_hif2;
 		}
@@ -815,7 +858,7 @@ int mt7996_dma_init(struct mt7996_dev *d
 		}
 	}
 
-	if (dev->has_rro) {
+	if (dev->mt76.hwrro_mode) {
 		/* rx rro data queue for band0 */
 		dev->mt76.q_rx[MT_RXQ_RRO_BAND0].flags =
 			MT_WED_RRO_Q_DATA(0) | MT_QFLAG_WED_RRO_EN;
@@ -950,9 +993,9 @@ void mt7996_dma_reset(struct mt7996_dev
 		dev_info(dev->mt76.dev,"%s L1 SER rx queue clean up done.",
 			 wiphy_name(dev->mt76.hw->wiphy));
 	#ifdef CONFIG_NET_MEDIATEK_SOC_WED
-	if (dev->has_rro && !mtk_wed_device_active(&dev->mt76.mmio.wed)) 
+	if (dev->mt76.hwrro_mode && !mtk_wed_device_active(&dev->mt76.mmio.wed)) 
 	#else
-	if (dev->has_rro) 
+	if (dev->mt76.hwrro_mode)
 	#endif
 	{
 		mt7996_rro_msdu_pg_free(dev);
@@ -968,6 +1011,7 @@ void mt7996_dma_reset(struct mt7996_dev
 	/* reset wfsys */
 	if (force)
 		mt7996_wfsys_reset(dev);
+
 #ifdef CONFIG_NET_MEDIATEK_SOC_WED
 	if (dev->hif2 && mtk_wed_device_active(&dev->mt76.mmio.wed_hif2))
 		mtk_wed_device_dma_reset(&dev->mt76.mmio.wed_hif2);
--- a/mt7996/init.c
+++ b/mt7996/init.c
@@ -519,10 +519,8 @@ mt7996_init_wiphy(struct ieee80211_hw *h
 	hw->max_rx_aggregation_subframes = max_subframes;
 	hw->max_tx_aggregation_subframes = max_subframes;
 	hw->netdev_features = NETIF_F_RXCSUM;
-#ifdef CONFIG_NET_MEDIATEK_SOC_WED		
-	if (mtk_wed_device_active(wed))
+	if (mtk_wed_device_active(wed) || mt76_npu_device_active(mdev))
 		hw->netdev_features |= NETIF_F_HW_TC;
-#endif
 	hw->radiotap_timestamp.units_pos =
 		IEEE80211_RADIOTAP_TIMESTAMP_UNIT_US;
 
@@ -709,11 +707,11 @@ void mt7996_mac_init(struct mt7996_dev *
 	}
 
 	/* griffin does not have WA */
-	if (!dev->has_rro && mt7996_has_wa(dev))
+	if (!dev->mt76.hwrro_mode && mt7996_has_wa(dev))
 		txfree_path = MT7996_TXFREE_FROM_WA;
 
 	rx_path_type = dev->hif2 ? rx_path_type : 0;
-	rro_bypass = dev->has_rro ? rro_bypass : MT7996_RRO_ALL_BYPASS;
+	rro_bypass = dev->mt76.hwrro_mode ? rro_bypass : MT7996_RRO_ALL_BYPASS;
 
 	mt7996_mcu_set_rro(dev, UNI_RRO_SET_PLATFORM_TYPE, rx_path_type);
 	mt7996_mcu_set_rro(dev, UNI_RRO_SET_BYPASS_MODE, rro_bypass);
@@ -723,7 +721,7 @@ void mt7996_mac_init(struct mt7996_dev *
 		"Platform_type = %d, bypass_rro = %d, txfree_path = %d\n",
 		rx_path_type, rro_bypass, txfree_path);
 
-	if (dev->has_rro) {
+	if (dev->mt76.hwrro_mode) {
 		u16 timeout;
 
 		timeout = mt76_rr(dev, MT_HW_REV) == MT_HW_REV1 ? 512 : 128;
@@ -958,6 +956,33 @@ void mt7996_wfsys_reset(struct mt7996_de
 	msleep(20);
 }
 
+
+/*rro3.1*/
+static void mt7996_rro_3_1_hw_init(struct mt7996_dev *dev)
+{
+	u32 reg = MT_RRO_MSDU_PG_SEG_ADDR0;
+	int i;
+
+
+	mt76_set(dev, MT_RRO_3_1_GLOBAL_CONFIG,
+		 MT_RRO_3_1_GLOBAL_CONFIG_INTERLEAVE_EN);
+	/* setup Msdu page address */
+	for (i = 0; i < MT7996_RRO_MSDU_PG_CR_CNT; i++) {
+		mt76_wr(dev, reg, dev->wed_rro.msdu_pg[i].phy_addr >> 4);
+		reg += 4;
+	}
+
+	mt76_clear(dev, MT_RRO_3_0_EMU_CONF,
+			MT_RRO_3_0_EMU_CONF_EN_MASK);
+	mt76_set(dev, MT_RRO_3_1_GLOBAL_CONFIG,
+			MT_RRO_3_1_GLOBAL_CONFIG_RXDMAD_SEL);
+
+	/* interrupt enable */
+	mt76_wr(dev, MT_RRO_HOST_INT_ENA,
+		MT_RRO_HOST_INT_ENA_HOST_RRO_DONE_ENA);
+}
+
+
 void mt7996_rro_hw_init(struct mt7996_dev *dev)
 {
 #ifdef CONFIG_NET_MEDIATEK_SOC_WED
@@ -966,13 +991,18 @@ void mt7996_rro_hw_init(struct mt7996_de
 	u32 reg = MT_RRO_ADDR_ELEM_SEG_ADDR0;
 	int i;
 
-	if (!dev->has_rro)
+	if (!dev->mt76.hwrro_mode)
 		return;
 
 	INIT_LIST_HEAD(&dev->wed_rro.pg_addr_cache);
 	for (i = 0; i < MT7996_RRO_MSDU_PG_HASH_SIZE; i++)
 		INIT_LIST_HEAD(&dev->wed_rro.pg_hash_head[i]);
 
+	/*rro3.1*/
+	if (dev->mt76.hwrro_mode == MT76_HWRRO_V3_1)
+		return mt7996_rro_3_1_hw_init(dev);
+
+
 	if (!is_mt7996(&dev->mt76)) {
 		/* set emul 3.0 function */
 		mt76_wr(dev, MT_RRO_3_0_EMU_CONF,
@@ -1057,23 +1087,24 @@ static int mt7996_wed_rro_init(struct mt
 {
 #ifdef CONFIG_NET_MEDIATEK_SOC_WED	
 	struct mtk_wed_device *wed = &dev->mt76.mmio.wed;
-
+#endif
 	struct mt7996_wed_rro_addr *addr;
 	void *ptr;
 	int i;
 
-	if (!dev->has_rro)
+	if (!dev->mt76.hwrro_mode)
 		return 0;
+	if (dev->mt76.hwrro_mode == MT76_HWRRO_V3) {
+		for (i = 0; i < ARRAY_SIZE(dev->wed_rro.ba_bitmap); i++) {
+			ptr = dmam_alloc_coherent(dev->mt76.dma_dev,
+						  MT7996_RRO_BA_BITMAP_CR_SIZE,
+						  &dev->wed_rro.ba_bitmap[i].phy_addr,
+						  GFP_KERNEL);
+			if (!ptr)
+				return -ENOMEM;
 
-	for (i = 0; i < ARRAY_SIZE(dev->wed_rro.ba_bitmap); i++) {
-		ptr = dmam_alloc_coherent(dev->mt76.dma_dev,
-					  MT7996_RRO_BA_BITMAP_CR_SIZE,
-					  &dev->wed_rro.ba_bitmap[i].phy_addr,
-					  GFP_KERNEL);
-		if (!ptr)
-			return -ENOMEM;
-
-		dev->wed_rro.ba_bitmap[i].ptr = ptr;
+			dev->wed_rro.ba_bitmap[i].ptr = ptr;
+		}
 	}
 
 	for (i = 0; i < ARRAY_SIZE(dev->wed_rro.addr_elem); i++) {
@@ -1095,6 +1126,7 @@ static int mt7996_wed_rro_init(struct mt
 			addr->signature = 0xff;
 			addr++;
 		}
+
 #ifdef CONFIG_NET_MEDIATEK_SOC_WED		
 		if (mtk_wed_device_active(wed) && mtk_wed_get_rx_capa(wed))
 			wed->wlan.ind_cmd.addr_elem_phys[i] = dev->wed_rro.addr_elem[i].phy_addr;
@@ -1130,16 +1162,13 @@ static int mt7996_wed_rro_init(struct mt
 	mt7996_rro_hw_init(dev);
 
 	return mt7996_dma_rro_init(dev);
-#else
-	return 0;
-#endif
 }
 
 static void mt7996_wed_rro_free(struct mt7996_dev *dev)
 {
 	int i;
 
-	if (!dev->has_rro)
+	if (!dev->mt76.hwrro_mode)
 		return;
 
 	for (i = 0; i < ARRAY_SIZE(dev->wed_rro.ba_bitmap); i++) {
@@ -1203,6 +1232,9 @@ static void mt7996_wed_rro_work(struct w
 				     list);
 		list_del_init(&e->list);
 
+		if (mt76_npu_device_active(&dev->mt76))
+			goto reset_session;
+
 		for (i = 0; i < MT7996_RRO_WINDOW_MAX_LEN; i++) {
 			void *ptr = dev->wed_rro.session.ptr;
 			struct mt7996_wed_rro_addr *elem;
@@ -1223,6 +1255,7 @@ reset:
 			elem = ptr + elem_id * sizeof(*elem);
 			elem->signature = 0xff;
 		}
+reset_session:
 		mt7996_mcu_wed_rro_reset_sessions(dev, e->id);
 out:
 		kfree(e);
@@ -1898,7 +1931,7 @@ void mt7996_unregister_device(struct mt7
 	#ifdef CONFIG_NET_MEDIATEK_SOC_WED	
 	if (dev->has_rro && !mtk_wed_device_active(&dev->mt76.mmio.wed)) 
 	#else
-	if(dev->has_rro)
+	if (dev->mt76.hwrro_mode)  //rro3.1
 	{
 		mt7996_rro_msdu_pg_free(dev);
 		mt7996_rx_token_put(dev);
--- a/mt7996/mac.c
+++ b/mt7996/mac.c
@@ -333,6 +333,9 @@ mt7996_mac_fill_rx(struct mt7996_dev *de
 
 	band_idx = FIELD_GET(MT_RXD1_NORMAL_BAND_IDX, rxd1);
 	mphy = dev->mt76.phys[band_idx];
+	if (!mphy)
+		return -EINVAL;
+
 	phy = mphy->priv;
 	status->phy_idx = mphy->band_idx;
 
@@ -952,6 +955,7 @@ int mt7996_tx_prepare_skb(struct mt76_de
 			  struct mt76_tx_info *tx_info)
 {
 	struct ieee80211_hdr *hdr = (struct ieee80211_hdr *)tx_info->skb->data;
+	int token_start = mt76_npu_device_active(mdev) ? MT7996_TOKEN_SIZE : 0;
 	struct mt7996_dev *dev = container_of(mdev, struct mt7996_dev, mt76);
 	struct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx_info->skb);
 	struct ieee80211_key_conf *key = info->control.hw_key;
@@ -1008,7 +1012,13 @@ int mt7996_tx_prepare_skb(struct mt76_de
 	t = (struct mt76_txwi_cache *)(txwi + mdev->drv->txwi_size);
 	t->skb = tx_info->skb;
 
-	id = mt76_token_consume(mdev, &t, mconf->mt76.band_idx);
+#ifdef CONFIG_NET_MEDIATEK_SOC_WED
+	if (mtk_wed_device_active(&mdev->mmio.wed))
+		token_start = mdev->mmio.wed.wlan.nbuf;
+#endif
+
+	id = __mt76_token_consume(mdev, &t, mconf->mt76.band_idx,
+				  token_start);
 	if (id < 0) {
 		mdev->tx_dbg_stats.tx_drop[MT_TX_DROP_GET_TOKEN_FAIL]++;
 		return id;
@@ -1228,6 +1238,41 @@ out:
 	mt76_put_txwi(mdev, t);
 }
 
+
+#ifdef CONFIG_NET_AIROHA_NPU
+void npu_mt7996_txwi_free(struct mt76_dev *dev, struct mt76_txwi_cache *t,
+		 struct ieee80211_sta *sta, struct mt76_wcid *wcid,
+		 struct list_head *free_list)
+{
+	struct mt76_dev *mdev = dev;
+	__le32 *txwi;
+	u16 wcid_idx;
+
+	mt76_connac_txp_skb_unmap(mdev, t);
+	if (!t->skb)
+		goto out;
+
+	txwi = (__le32 *)mt76_get_txwi_ptr(mdev, t);
+	if (sta) {
+		wcid_idx = wcid->idx;
+
+		if (likely(t->skb->protocol != cpu_to_be16(ETH_P_PAE)))
+			mt7996_tx_check_aggr(sta, t->skb, wcid);
+	} else {
+		wcid_idx = le32_get_bits(txwi[9], MT_TXD9_WLAN_IDX);
+	}
+
+	__mt76_tx_complete_skb(mdev, wcid_idx, t->skb, free_list);
+
+out:
+	t->skb = NULL;
+	mt76_put_txwi(mdev, t);
+}
+
+EXPORT_SYMBOL_GPL(npu_mt7996_txwi_free);
+#endif
+
+
 static void
 mt7996_mac_tx_free(struct mt7996_dev *dev, void *data, int len)
 {
@@ -1966,6 +2011,103 @@ update_ack_sn:
 			FIELD_PREP(MT_RRO_ACK_SN_CTRL_SN_MASK, sn));
 }
 
+
+/*rro3.1*/
+void mt7996_rx_init_rxdmad_c(struct mt76_dev *mdev, struct mt76_queue *q)
+{
+	struct mt76_desc *desc;
+	int i;
+
+	q->magic_cnt = 0;
+	desc = (struct mt76_desc *)q->desc;
+	for (i = 0; i < q->ndesc; i++) {
+		struct mt76_rro_rxdmad_c *dmad;
+
+		dmad = (struct mt76_rro_rxdmad_c *)&desc[i];
+		dmad->magic_cnt = MT_DMA_MAGIC_CNT - 1;
+	}
+}
+
+/*rro3.1*/
+void mt7996_rro_rxdamdc_process(struct mt76_dev *mdev, void *data)
+{
+	struct mt76_rro_rxdmad_c *dmad = (struct mt76_rro_rxdmad_c *)data;
+	struct mt76_rxwi_cache *r;
+	//struct mt76_rx_status *status;
+	struct mt76_queue *q;
+	struct sk_buff *skb;
+	int len, data_len;
+	void *buf;
+	u8 more, qid;
+	u32 info = 0;
+
+	r = mt76_rx_token_release(mdev, dmad->rx_token_id);
+	len = dmad->sdl0;
+	more = !dmad->ls;
+	if (!r)
+		return;
+
+	qid = r->qid;
+	buf = r->ptr;
+	q = &mdev->q_rx[qid];
+	dma_unmap_single(mdev->dma_dev, r->dma_addr,
+			 SKB_WITH_OVERHEAD(q->buf_size),
+			 DMA_FROM_DEVICE);
+
+	r->dma_addr = 0;
+	r->ptr = NULL;
+	mt76_put_rxwi(mdev, r);
+	if (!buf)
+		return;
+
+	if (q->rx_head)
+		data_len = q->buf_size;
+	else
+		data_len = SKB_WITH_OVERHEAD(q->buf_size);
+
+	if (data_len < len + q->buf_offset) {
+		dev_kfree_skb(q->rx_head);
+		skb_free_frag(buf);
+		q->rx_head = NULL;
+		return;
+	}
+
+	if (q->rx_head) {
+		/* TDO: fragment error, skip handle */
+		//mt76_add_fragment(mdev, q, buf, len, more, info);
+		skb_free_frag(buf);
+		if (!more) {
+			dev_kfree_skb(q->rx_head);
+			q->rx_head = NULL;
+		}
+		return;
+	}
+
+	if (!more && !mt7996_rx_check(mdev, buf, len))
+		return;
+
+	skb = build_skb(buf, q->buf_size);
+	if (!skb)
+		return;
+
+	skb_reserve(skb, q->buf_offset);
+	__skb_put(skb, len);
+
+	if (dmad->ind_reason == 1 || dmad->ind_reason == 2) {
+		dev_kfree_skb(skb);
+		return;
+	}
+
+	if (more) {
+		q->rx_head = skb;
+		return;
+	}
+
+	mt7996_queue_rx_skb(mdev, qid, skb, &info);
+
+}
+
+
 void mt7996_mac_cca_stats_reset(struct mt7996_phy *phy)
 {
 	struct mt7996_dev *dev = phy->dev;
@@ -2251,8 +2393,9 @@ mt7996_mac_restart(struct mt7996_dev *de
 		goto out;
 
 #ifdef CONFIG_NET_MEDIATEK_SOC_WED	
-	if (mtk_wed_device_active(&dev->mt76.mmio.wed) && dev->has_rro) {
+	if (mtk_wed_device_active(&dev->mt76.mmio.wed) && dev->mt76.hwrro_mode) {
 		u32 wed_irq_mask = dev->mt76.mmio.irqmask |
+					MT_INT_RRO_RX_DONE |
 				   MT_INT_TX_DONE_BAND2;
 
 		mt7996_rro_hw_init(dev);
@@ -2507,7 +2650,7 @@ void mt7996_mac_reset_work(struct work_s
 	dev_info(dev->mt76.dev,"%s L1 SER dma start done.",
 		 wiphy_name(dev->mt76.hw->wiphy));
 
-	if (is_mt7992(&dev->mt76) && dev->has_rro)
+	if (!is_mt7996(&dev->mt76) && dev->mt76.hwrro_mode == MT76_HWRRO_V3)  //rro3.1
 		mt76_wr(dev, MT_RRO_3_0_EMU_CONF, MT_RRO_3_0_EMU_CONF_EN_MASK);
 
 	#ifdef CONFIG_NET_MEDIATEK_SOC_WED	
@@ -2570,7 +2713,6 @@ void mt7996_mac_reset_work(struct work_s
 		ieee80211_wake_queues(phy3->mt76->hw);
 
 	mt7996_update_beacons(dev);
-
 	dev_info(dev->mt76.dev,"\n%s L1 SER recovery completed.",
 		 wiphy_name(dev->mt76.hw->wiphy));
 }
--- a/mt7996/main.c
+++ b/mt7996/main.c
@@ -2564,7 +2564,6 @@ out:
 	return ret;
 }
 
-#ifdef CONFIG_NET_MEDIATEK_SOC_WED
 static int
 mt7996_net_fill_forward_path(struct ieee80211_hw *hw,
 			     struct ieee80211_vif *vif,
@@ -2612,15 +2611,18 @@ mt7996_net_fill_forward_path(struct ieee
 			break;
 		}
 	}
-	#ifdef CONFIG_NET_MEDIATEK_SOC_WED	
-	if (!mtk_wed_device_active(wed))
-	#else
+
+	if (!mtk_wed_device_active(wed) && !mt76_npu_device_active(&dev->mt76))
 		return -ENODEV;
-	#endif
 
 	path->type = DEV_PATH_MTK_WDMA;
 	path->dev = ctx->dev;
-	path->mtk_wdma.wdma_idx = wed->wdma_idx;
+#ifdef CONFIG_NET_MEDIATEK_SOC_WED
+	if (mtk_wed_device_active(wed))
+		path->mtk_wdma.wdma_idx = wed->wdma_idx;
+	else
+#endif
+	path->mtk_wdma.wdma_idx = mconf->phy->mt76->band_idx;
 	path->mtk_wdma.bss = mconf->mt76.idx;
 	path->mtk_wdma.queue = 0;
 	path->mtk_wdma.wcid = mlink->wcid.idx;
@@ -2642,8 +2644,6 @@ mt7996_net_fill_forward_path(struct ieee
 	return 0;
 }
 
-#endif
-
 void mt7996_scan_complete(struct mt7996_phy *phy, bool aborted)
 {
 	struct ieee80211_vif *vif = phy->scan_vif;
@@ -3513,9 +3513,11 @@ const struct ieee80211_ops mt7996_ops =
 	.vif_add_debugfs = mt7996_vif_add_debugfs,
 #endif
 	.set_radar_background = mt7996_set_radar_background,
-#ifdef CONFIG_NET_MEDIATEK_SOC_WED
 	.net_fill_forward_path = mt7996_net_fill_forward_path,
+#ifdef CONFIG_NET_MEDIATEK_SOC_WED
 	.net_setup_tc = mt76_wed_net_setup_tc,
+#elif (IS_BUILTIN(CONFIG_NET_AIROHA_NPU) || IS_MODULE(CONFIG_NET_AIROHA_NPU))
+	.net_setup_tc = mt76_npu_net_setup_tc,
 #endif
 	.event_callback = mt7996_event_callback,
 	.add_chanctx = mt7996_add_chanctx,
--- a/mt7996/mcu.c
+++ b/mt7996/mcu.c
@@ -1253,7 +1253,7 @@ mt7996_mcu_wed_rro_event(struct mt7996_d
 {
 	struct mt7996_mcu_wed_rro_event *event = (void *)skb->data;
 
-	if (!dev->has_rro)
+	if (!dev->mt76.hwrro_mode)  //rro3.1
 		return;
 
 	skb_pull(skb, sizeof(struct mt7996_mcu_rxd) + 4);
@@ -2132,7 +2132,7 @@ mt7996_mcu_sta_ba(struct mt7996_dev *dev
 	ba->ba_en = enable << params->tid;
 	ba->amsdu = params->amsdu;
 	ba->tid = params->tid;
-	ba->ba_rdd_rro = !tx && enable && dev->has_rro;
+	ba->ba_rdd_rro = !tx && enable && dev->mt76.hwrro_mode;
 
 	return mt76_mcu_skb_send_msg(&dev->mt76, skb,
 				     MCU_WMWA_UNI_CMD(STA_REC_UPDATE), true);
--- a/mt7996/mmio.c
+++ b/mt7996/mmio.c
@@ -462,7 +462,7 @@ int mt7996_mmio_wed_init(struct mt7996_d
 	if (!wed_enable)
 		return 0;
 
-	dev->has_rro = true;
+	dev->mt76.hwrro_mode = MT76_HWRRO_V3_1;  //rro3.1
 
 	if (dev->hif2)
 		hif1_ofs = MT_WFDMA0_PCIE1(0) - MT_WFDMA0(0);
@@ -483,10 +483,10 @@ int mt7996_mmio_wed_init(struct mt7996_d
 				      MT_INT_PCIE1_SOURCE_CSR_EXT;
 		wed->wlan.wpdma_mask = wed->wlan.phy_base +
 				       MT_INT_PCIE1_MASK_CSR;
-		wed->wlan.wpdma_tx = wed->wlan.phy_base + hif1_ofs +
+		wed->wlan.wpdma_tx[0] = wed->wlan.phy_base + hif1_ofs + 
 					     MT_TXQ_RING_BASE(0) +
 					     MT7996_TXQ_BAND2 * MT_RING_SIZE;
-		if (dev->has_rro) {
+		if (dev->mt76.hwrro_mode) { 
 			switch (mt76_chip(&dev->mt76)) {
 			case MT7996_DEVICE_ID:
 				intr = MT_INT_RX_TXFREE_EXT;
@@ -523,10 +523,10 @@ int mt7996_mmio_wed_init(struct mt7996_d
 		wed->wlan.id = MT7996_DEVICE_ID_2;
 		wed->wlan.tx_tbit[0] = ffs(MT_INT_TX_DONE_BAND2) - 1;
 	} else {
-		wed->wlan.hw_rro = dev->has_rro; /* default on */
+		wed->wlan.hwrro_mode = dev->mt76.hwrro_mode; /* default on */ //rro3.1
 		wed->wlan.wpdma_int = wed->wlan.phy_base + MT_INT_SOURCE_CSR;
 		wed->wlan.wpdma_mask = wed->wlan.phy_base + MT_INT_MASK_CSR;
-		wed->wlan.wpdma_tx = wed->wlan.phy_base + MT_TXQ_RING_BASE(0) +
+		wed->wlan.wpdma_tx[0] = wed->wlan.phy_base + MT_TXQ_RING_BASE(0) +
 				     MT7996_TXQ_BAND0 * MT_RING_SIZE;
 
 		wed->wlan.wpdma_rx_glo = wed->wlan.phy_base + MT_WFDMA0_GLO_CFG;
@@ -578,7 +578,7 @@ int mt7996_mmio_wed_init(struct mt7996_d
 		wed->wlan.tx_tbit[1] = ffs(MT_INT_TX_DONE_BAND1) - 1;
 		switch (mt76_chip(&dev->mt76)) {
 		case MT7996_DEVICE_ID:
-			if (dev->has_rro) {
+			if (dev->mt76.hwrro_mode) {
 				intr = MT_INT_RX_TXFREE_MAIN;
 				ring = MT7996_RXQ_TXFREE0;
 			} else {
@@ -624,7 +624,7 @@ int mt7996_mmio_wed_init(struct mt7996_d
 
 	if (mtk_wed_device_attach(wed)) {
 		wed_enable = false;
-		dev->has_rro = false;
+		dev->mt76.hwrro_mode = MT76_HWRRO_DISABLE;
 		return 0;
 	}
 
@@ -724,9 +724,18 @@ void mt7996_dual_hif_set_irq_mask(struct
 static void mt7996_rx_poll_complete(struct mt76_dev *mdev,
 				    enum mt76_rxq_id q)
 {
-	struct mt7996_dev *dev = container_of(mdev, struct mt7996_dev, mt76);
+	if (q == MT_RXQ_NPU0 || q == MT_RXQ_NPU1) {
+		struct airoha_npu *npu;
 
-	mt7996_irq_enable(dev, MT_INT_RX(q));
+		npu = rcu_dereference(mdev->mmio.npu);
+		if (npu)
+			npu->ops.wlan_irq_enable(npu, q - MT_RXQ_NPU0);
+	} else {
+		struct mt7996_dev *dev = container_of(mdev, struct mt7996_dev,
+						      mt76);
+
+		mt7996_irq_enable(dev, MT_INT_RX(q));
+	}
 }
 
 /* TODO: support 2/4/6/8 MSI-X vectors */
@@ -849,11 +858,15 @@ struct mt7996_dev *mt7996_mmio_probe(str
 		.rx_poll_complete = mt7996_rx_poll_complete,
 		.rx_rro_ind_process = mt7996_rro_rx_process,
 		.rx_rro_fill_msdu_pg = mt7996_rro_fill_msdu_page,
+		/*rro3.1*/
+		.rx_init_rxdmad_c = mt7996_rx_init_rxdmad_c,
+		.rx_rro_rxdmadc_process = mt7996_rro_rxdamdc_process,
+		/***/	
 		.sta_add = mt7996_mac_sta_add,
 		.sta_event = mt7996_mac_sta_event,
 		.sta_remove = mt7996_mac_sta_remove,
 		.update_survey = mt7996_update_channel,
-		.set_channel = mt7996_set_channel,
+		.set_channel = mt7996_set_channel,	
 	};
 	struct mt7996_dev *dev;
 	struct mt76_dev *mdev;
@@ -872,7 +885,6 @@ struct mt7996_dev *mt7996_mmio_probe(str
 	tasklet_setup(&mdev->irq_tasklet, mt7996_irq_tasklet);
 
 	mt76_wr(dev, MT_INT_MASK_CSR, 0);
-
 	return dev;
 
 error:
--- a/mt7996/mt7996.h
+++ b/mt7996/mt7996.h
@@ -269,6 +269,7 @@ enum mt7996_rxq_id {
 	MT7996_RXQ_TXFREE1 = 9,
 	MT7996_RXQ_TXFREE2 = 7,
 	MT7996_RXQ_RRO_IND = 0,
+	MT7996_RXQ_RRO_RXDMAD_C = 0,
 	MT7990_RXQ_TXFREE0 = 6,
 	MT7990_RXQ_TXFREE1 = 7,
 };
@@ -1454,11 +1455,21 @@ int mt7996_tx_prepare_skb(struct mt76_de
 void mt7996_tx_token_put(struct mt7996_dev *dev);
 void mt7996_queue_rx_skb(struct mt76_dev *mdev, enum mt76_rxq_id q,
 			 struct sk_buff *skb, u32 *info);
+#ifdef CONFIG_NET_AIROHA_NPU
+void npu_mt7996_txwi_free(struct mt76_dev *dev, struct mt76_txwi_cache *t,
+		 struct ieee80211_sta *sta, struct mt76_wcid *wcid,	 struct list_head *free_list);
+#endif
 void mt7996_rx_token_put(struct mt7996_dev *dev);
 void mt7996_rro_msdu_pg_free(struct mt7996_dev *dev);
 void mt7996_rro_rx_process(struct mt76_dev *mdev, void *data);
 int mt7996_rro_fill_msdu_page(struct mt76_dev *mdev, struct mt76_queue *q,
 			      dma_addr_t p, void *data);
+
+/*rro3.1*/
+void mt7996_rx_init_rxdmad_c(struct mt76_dev *mdev, struct mt76_queue *q);
+void mt7996_rro_rxdamdc_process(struct mt76_dev *mdev, void *data);
+/**/
+
 bool mt7996_rx_check(struct mt76_dev *mdev, void *data, int len);
 void mt7996_stats_work(struct work_struct *work);
 void mt7996_scan_work(struct work_struct *work);
@@ -1587,8 +1598,8 @@ int mt7996_mcu_set_uba_en(struct mt7996_
 int mt7996_mcu_set_mru_probe_en(struct mt7996_phy *phy);
 #endif
 
-#ifdef CONFIG_NET_MEDIATEK_SOC_WED
 int mt7996_dma_rro_init(struct mt7996_dev *dev);
+#ifdef CONFIG_NET_MEDIATEK_SOC_WED
 extern void mtk_set_pse_drop(u32 config);
 #endif /* CONFIG_NET_MEDIATEK_SOC_WED */
 
--- a/mt7996/mtk_debugfs.c
+++ b/mt7996/mtk_debugfs.c
@@ -250,6 +250,12 @@ static int mt7996_amsdu_result_read(stru
 	return 0;
 }
 
+static int npu_support_map_read(struct seq_file *s, void *data)
+{
+	return 0;
+}
+
+
 /* DBG MODLE */
 static int
 mt7996_fw_debug_module_set(void *data, u64 module)
@@ -4641,6 +4647,9 @@ void mt7996_mtk_init_dev_debugfs(struct
 	debugfs_create_devm_seqfile(dev->mt76.dev, "wtbl_info", dir,
 				    mt7996_wtbl_read);
 
+	debugfs_create_devm_seqfile(dev->mt76.dev, "npu_support_map", dir,
+				    npu_support_map_read);	
+
 	debugfs_create_devm_seqfile(dev->mt76.dev, "token", dir, mt7996_token_read);
 	debugfs_create_file("red", 0200, dir, dev, &fops_red_config);
 	debugfs_create_file("vow_drr_dbg", 0200, dir, dev, &fops_vow_drr_dbg);
--- a/mt7996/pci.c
+++ b/mt7996/pci.c
@@ -13,8 +13,8 @@
 static bool hif2_enable = false;
 module_param(hif2_enable, bool, 0644);
 
-static bool rro_enable = false;
-module_param(rro_enable, bool, 0644);
+static int rro_mode = MT76_HWRRO_V3_1;
+module_param(rro_mode, int, 0644);
 
 static LIST_HEAD(hif_list);
 static DEFINE_SPINLOCK(hif_lock);
@@ -107,6 +107,111 @@ static int mt7996_pci_hif2_probe(struct
 	return 0;
 }
 
+static void mt7996_npu_wlan_offload_init(struct mt7996_dev *dev,
+					 struct airoha_npu *npu,
+					 struct pci_dev *pdev)
+{
+	int i, type = pdev->bus && pci_domain_nr(pdev->bus) ? 3 : 2;
+	phys_addr_t paddr = pci_resource_start(pdev, 0);
+	u32 hif1_ofs = 0;
+
+	if (npu->ops.wlan_set_pcie_port_type(npu, 0, type))
+		dev_warn(dev->mt76.dev,
+			 "failed setting npu wlan PCIe port type\n");
+
+	if (!is_mt7992(&dev->mt76))
+		return;
+
+	if (dev->hif2)
+		hif1_ofs = MT_WFDMA0_PCIE1(0) - MT_WFDMA0(0);
+
+	for (i = MT_BAND0; i < MT_BAND2; i++) {
+		u32 addr = paddr;
+
+		if (i)
+			addr += MT_RXQ_RING_BASE(MT_RXQ_RRO_BAND1) + 0x90 +
+				hif1_ofs;
+		else
+			addr += MT_RXQ_RING_BASE(MT_RXQ_RRO_BAND0) + 0x80;
+
+		if (npu->ops.wlan_set_pcie_addr(npu, i, addr))
+			dev_warn(dev->mt76.dev,
+				 "failed setting npu wlan PCIe desc addr\n");
+
+		if (npu->ops.wlan_set_desc(npu, i, MT7996_RX_RING_SIZE))
+			dev_warn(dev->mt76.dev,
+				 "failed setting npu wlan PCIe desc size\n");
+
+		addr = paddr;
+		if (i)
+			addr += MT_TXQ_RING_BASE(0) + 0x150 + hif1_ofs;
+		else
+			addr += MT_TXQ_RING_BASE(0) + 0x120;
+
+		if (npu->ops.wlan_set_tx_ring_pcie_addr(npu, i, addr))
+			dev_warn(dev->mt76.dev,
+				 "failed setting npu wlan tx desc addr\n");
+	}
+
+	/*rxdmad_c ring*/
+	if (npu->ops.wlan_set_pcie_addr(npu, 9,
+					paddr + MT_RXQ_RRO_AP_RING_BASE))
+		dev_warn(dev->mt76.dev,
+			 "failed setting npu wlan rxdmad_c addr\n");
+
+	if (npu->ops.wlan_set_desc(npu, 9, MT7996_RX_RING_SIZE))
+		dev_warn(dev->mt76.dev,
+			 "failed setting npu wlan rxdmad_c desc size\n");
+
+	if (npu->ops.wlan_set_tx_ring_pcie_addr(npu, 2,
+						paddr + MT_RRO_ACK_SN_CTRL))
+		dev_warn(dev->mt76.dev,
+			 "failed setting npu wlan rro_ack_sn desc addr\n");
+}
+
+static void mt7996_npu_init_rx_event(struct mt7996_dev *dev,
+				     struct airoha_npu *npu,
+				     struct pci_dev *pdev)
+{
+	struct mt76_queue *q =  &dev->mt76.q_rx[MT_RXQ_MAIN_WA];
+	phys_addr_t paddr = pci_resource_start(pdev, 0);
+
+	if (npu->ops.wlan_set_rx_ring_for_txdone(npu, 0, q->desc_dma))
+		dev_warn(dev->mt76.dev,
+			 "failed setting npu wlan tx-done ring\n");
+
+	if (npu->ops.wlan_set_desc(npu, 10, MT7996_RX_MCU_RING_SIZE))
+		dev_warn(dev->mt76.dev,
+			 "failed setting npu wlan descriptors\n");
+
+	paddr += MT_RXQ_RING_BASE(MT_RXQ_MAIN_WA) + 0x20;
+	if (npu->ops.wlan_set_pcie_addr(npu, 10, paddr))
+		dev_warn(dev->mt76.dev,
+			 "failed setting npu wlan rx pcie address\n");
+}
+
+static void mt7996_npu_pci_hw_init(struct mt7996_dev *dev, struct pci_dev *pdev)
+{
+	struct airoha_npu *npu;
+	int i;
+
+	rcu_read_lock();
+	npu = rcu_dereference(dev->mt76.mmio.npu);
+	if (!npu)
+		goto out;
+
+	mt7996_npu_wlan_offload_init(dev, npu, pdev);
+	mt76_npu_init_rxd(&dev->mt76, npu);
+	mt76_npu_init_txd(&dev->mt76, npu, MT7996_TX_RING_SIZE);
+	mt7996_npu_init_rx_event(dev, npu, pdev);
+	mt76_npu_init_tx_done(&dev->mt76, npu);
+
+	for (i = MT_RXQ_NPU0; i <= MT_RXQ_NPU1; i++)
+		npu->ops.wlan_irq_enable(npu, i - MT_RXQ_NPU0);
+out:
+	rcu_read_unlock();
+}
+
 static int mt7996_pci_probe(struct pci_dev *pdev,
 			    const struct pci_device_id *id)
 {
@@ -114,8 +219,7 @@ static int mt7996_pci_probe(struct pci_d
 	struct mt7996_hif *hif2;
 	struct mt7996_dev *dev;
 	int irq, ret;
-	struct mt76_dev *mdev;
-
+	struct mt76_dev *mdev;	
 	hif2_enable |= (id->device == MT7996_DEVICE_ID ||
 			id->device == MT7996_DEVICE_ID_2 ||
 			id->device == MT7992_DEVICE_ID_2 ||
@@ -151,13 +255,28 @@ static int mt7996_pci_probe(struct pci_d
 	if (IS_ERR(dev))
 		return PTR_ERR(dev);
 
-	dev->has_rro = rro_enable;
+	//dev->has_rro = rro_enable;
 	mdev = &dev->mt76;
+	switch (rro_mode) {
+	case MT76_HWRRO_DISABLE:
+	case MT76_HWRRO_V3:
+		mdev->hwrro_mode = rro_mode;
+		break;
+	case MT76_HWRRO_V3_1:
+		mdev->hwrro_mode = is_mt7996(mdev) ? MT76_HWRRO_V3 : MT76_HWRRO_V3_1;
+		break;
+	default:
+		mdev->hwrro_mode = MT76_HWRRO_DISABLE;
+		break;
+	}	
 	mt7996_wfsys_reset(dev);
 	hif2 = mt7996_pci_init_hif2(pdev);
 	if (hif2)
 		dev->hif2 = hif2;
 
+	if (mt76_npu_init(mdev))
+		dev_warn(mdev->dev, "failed loading npu\n");
+
 	ret = mt7996_mmio_wed_init(dev, pdev, false, &irq);
 	if (ret < 0)
 		goto free_wed_or_irq_vector;
@@ -174,7 +293,7 @@ static int mt7996_pci_probe(struct pci_d
 			       IRQF_SHARED, KBUILD_MODNAME, dev);
 	if (ret)
 		goto free_wed_or_irq_vector;
-
+	
 	mt76_wr(dev, MT_INT_MASK_CSR, 0);
 	/* master switch of PCIe tnterrupt enable */
 	mt76_wr(dev, MT_PCIE_MAC_INT_ENABLE, 0xff);
@@ -214,6 +333,8 @@ static int mt7996_pci_probe(struct pci_d
 	if (ret)
 		goto free_hif2_irq;
 
+	mt7996_npu_pci_hw_init(dev, pdev);
+
 	return 0;
 
 free_hif2_irq:
--- a/mt7996/regs.h
+++ b/mt7996/regs.h
@@ -115,6 +115,7 @@ enum offs_rev {
 
 #define MT_RRO_3_1_GLOBAL_CONFIG		MT_RRO_TOP(0x604)
 #define MT_RRO_3_1_GLOBAL_CONFIG_INTERLEAVE_EN	BIT(0)
+#define MT_RRO_3_1_GLOBAL_CONFIG_RXDMAD_SEL	BIT(6)
 
 #define MT_RRO_MSDU_PG_SEG_ADDR0		MT_RRO_TOP(0x620)
 
@@ -511,6 +512,7 @@ enum offs_rev {
 #define MT_TXQ_RING_BASE(q)			(MT_Q_BASE(__TXQ(q)) + 0x300)
 #define MT_RXQ_RING_BASE(q)			(MT_Q_BASE(__RXQ(q)) + 0x500)
 #define MT_RXQ_RRO_IND_RING_BASE		MT_RRO_TOP(0x40)
+#define MT_RXQ_RRO_AP_RING_BASE			MT_RRO_TOP(0x650)
 
 #define MT_MCUQ_EXT_CTRL(q)			(MT_Q_BASE(q) +	0x600 +	\
 						 MT_MCUQ_ID(q) * 0x4)
@@ -547,6 +549,8 @@ enum offs_rev {
 #define MT_INT_RX_DONE_RRO_BAND1		BIT(17)
 #define MT_INT_RX_DONE_RRO_BAND2		BIT(14)
 #define MT_INT_RX_DONE_RRO_IND			BIT(11)
+#define MT_INT_RX_DONE_RRO_RXDMAD_C		BIT(11)   /*rro3.1*/
+
 #define MT_INT_RX_DONE_MSDU_PG_BAND0		BIT(18)
 #define MT_INT_RX_DONE_MSDU_PG_BAND1		BIT(19)
 #define MT_INT_RX_DONE_MSDU_PG_BAND2		BIT(23)
@@ -575,6 +579,7 @@ enum offs_rev {
 						 MT_INT_RX(MT_RXQ_RRO_BAND1) |		\
 						 MT_INT_RX(MT_RXQ_RRO_BAND2) |		\
 						 MT_INT_RX(MT_RXQ_RRO_IND) |		\
+						 MT_INT_RX(MT_RXQ_RRO_RXDMAD_C) |	\
 						 MT_INT_RX(MT_RXQ_MSDU_PAGE_BAND0) |	\
 						 MT_INT_RX(MT_RXQ_MSDU_PAGE_BAND1) |	\
 						 MT_INT_RX(MT_RXQ_MSDU_PAGE_BAND2))
--- /dev/null
+++ b/npu.c
@@ -0,0 +1,545 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (c) 2025 AIROHA Inc
+ * Author: Lorenzo Bianconi <lorenzo@kernel.org>
+ */
+#include <net/flow_offload.h>
+#include <net/pkt_cls.h>
+
+#include "mt76.h"
+#include "dma.h"
+#include "mt76_connac.h"
+
+#define MT76_NPU_RX_BUF_SIZE	(1800 + \
+				 SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
+
+static int mt76_npu_fill_rx_queue(struct mt76_dev *dev, struct mt76_queue *q)
+{
+	int nframes = 0;
+
+	/* FIXME: use page_pool here */
+	while (q->queued < q->ndesc - 1) {
+		struct mt76_queue_entry *e = &q->entry[q->head];
+		struct airoha_npu_rx_dma_desc *desc = q->desc;
+
+		e->buf = page_frag_alloc(&q->rx_page, q->buf_size,
+					 GFP_ATOMIC | GFP_DMA32);
+		if (!e->buf)
+			break;
+
+		e->dma_len[0] = SKB_WITH_OVERHEAD(q->buf_size);
+		e->dma_addr[0] = dma_map_single(dev->dma_dev, e->buf,
+						e->dma_len[0],
+						DMA_FROM_DEVICE);
+		if (dma_mapping_error(dev->dma_dev, e->dma_addr[0])) {
+			skb_free_frag(e->buf);
+			break;
+		}
+
+		memset(&desc[q->head], 0, sizeof(*desc));
+		desc[q->head].addr = e->dma_addr[0];
+
+		q->head = (q->head + 1) % q->ndesc;
+		q->queued++;
+		nframes++;
+	}
+
+	return nframes;
+}
+
+static void mt76_npu_queue_cleanup(struct mt76_dev *dev, struct mt76_queue *q)
+{
+	struct page *page;
+
+	spin_lock_bh(&q->lock);
+	while (q->queued > 0) {
+		struct mt76_queue_entry *e = &q->entry[q->tail];
+
+		dma_unmap_single(dev->dma_dev, e->dma_addr[0],
+				 e->dma_len[0], DMA_FROM_DEVICE);
+		skb_free_frag(e->buf);
+
+		q->tail = (q->tail + 1) % q->ndesc;
+		q->queued--;
+	}
+	spin_unlock_bh(&q->lock);
+
+	page = virt_to_page(q->rx_page.va);
+	__page_frag_cache_drain(page, q->rx_page.pagecnt_bias);
+	memset(&q->rx_page, 0, sizeof(q->rx_page));
+}
+
+static struct sk_buff *mt76_npu_dequeue(struct mt76_dev *dev,
+					struct airoha_ppe_dev *ppe_dev,
+					struct mt76_queue *q)
+{
+	struct airoha_npu_rx_dma_desc *desc = q->desc;
+	int i, nframes, index = q->tail;
+	struct sk_buff *skb = NULL;
+	u16 hash, reason;
+
+	nframes = FIELD_GET(NPU_RX_DMA_PKT_COUNT_MASK, desc[index].info);
+	nframes = max_t(int, nframes, 1);
+
+	for (i = 0; i < nframes; i++) {
+		struct mt76_queue_entry *e = &q->entry[index];
+		int len = FIELD_GET(NPU_RX_DMA_DESC_CUR_LEN_MASK,
+				    desc[index].ctrl);
+
+		if (!FIELD_GET(NPU_RX_DMA_DESC_DONE_MASK, desc[index].ctrl)) {
+			dev_kfree_skb(skb);
+			return NULL;
+		}
+
+		/* FIXME: use page_pool here */
+		dma_unmap_single(dev->dma_dev, e->dma_addr[0], e->dma_len[0],
+				 DMA_FROM_DEVICE);
+
+		if (!skb) {
+			skb = napi_build_skb(e->buf, q->buf_size);
+			if (!skb)
+				return NULL;
+
+			__skb_put(skb, len);
+			skb_reset_mac_header(skb);
+		} else {
+			struct skb_shared_info *shinfo = skb_shinfo(skb);
+			struct page *page = virt_to_head_page(e->buf);
+			int nr_frags = shinfo->nr_frags;
+
+			if (nr_frags < ARRAY_SIZE(shinfo->frags))
+				skb_add_rx_frag(skb, nr_frags, page,
+						e->buf - page_address(page),
+						len, q->buf_size);
+		}
+
+		hash = FIELD_GET(NPU_RX_DMA_FOE_ID_MASK, desc[index].info);
+		reason = FIELD_GET(NPU_RX_DMA_CRSN_MASK, desc[index].info);
+
+		index = (index + 1) % q->ndesc;
+	}
+	q->tail = index;
+	q->queued -= i;
+	Q_WRITE(q, dma_idx, q->tail);
+
+	if (skb) {
+		skb_set_hash(skb, hash, PKT_HASH_TYPE_L4);
+		if (reason == PPE_CPU_REASON_HIT_UNBIND_RATE_REACHED)
+			ppe_dev->ops.check_skb(ppe_dev, skb, hash, true);
+	}
+
+	return skb;
+}
+
+int mt76_npu_rx_poll(struct napi_struct *napi, int budget)
+{
+	struct airoha_ppe_dev *ppe_dev;
+	struct airoha_npu *npu;
+	enum mt76_rxq_id qid;
+	struct mt76_dev *dev;
+	int done = 0;
+
+	dev = container_of(napi->dev, struct mt76_dev, napi_dev);
+	qid = napi - dev->napi;
+
+	rcu_read_lock();
+
+	npu = rcu_dereference(dev->mmio.npu);
+	if (!npu)
+		goto out;
+
+	ppe_dev = rcu_dereference(dev->mmio.ppe_dev);
+	if (!ppe_dev)
+		goto out;
+
+	while (done < budget) {
+		struct sk_buff *skb;
+
+		skb = mt76_npu_dequeue(dev, ppe_dev, &dev->q_rx[qid]);
+		if (!skb)
+			break;
+
+		dev->drv->rx_skb(dev, qid, skb, 0);
+		mt76_rx_poll_complete(dev, qid, napi);
+		done++;
+	}
+
+	mt76_npu_fill_rx_queue(dev, &dev->q_rx[qid]);
+out:
+	if (done < budget && napi_complete(napi))
+		dev->drv->rx_poll_complete(dev, qid);
+
+	rcu_read_unlock();
+
+	return done;
+}
+EXPORT_SYMBOL_GPL(mt76_npu_rx_poll);
+
+static irqreturn_t mt76_npu_irq_handler(int irq, void *q_instance)
+{
+	struct mt76_queue *q = q_instance;
+	struct mt76_dev *dev = q->dev;
+	int qid = q - &dev->q_rx[0];
+	int index = qid - MT_RXQ_NPU0;
+	struct airoha_npu *npu;
+
+	rcu_read_lock();
+
+	npu = rcu_dereference(dev->mmio.npu);
+	if (!npu)
+		goto out;
+
+	/* FIXME: should we check status value */
+	//u32 status = npu->ops.wlan_get_irq(npu, index);
+
+	npu->ops.wlan_set_irq_mask(npu, index);
+
+	/* FIXME: should we check status value */
+	npu->ops.wlan_irq_disable(npu, index);
+	napi_schedule(&dev->napi[qid]);
+out:
+	rcu_read_unlock();
+
+	return IRQ_HANDLED;
+}
+
+int mt76_npu_dma_add_buf(struct mt76_phy *phy, struct mt76_queue *q,
+			 struct sk_buff *skb, struct mt76_queue_buf *buf,
+			 void *txwi_ptr)
+{
+	u16 txwi_len = min_t(u16, phy->dev->drv->txwi_size, NPU_TXWI_LEN);
+	struct airoha_npu_tx_dma_desc *desc = q->desc;
+	int ret;
+
+	/* FIXME: Take into account unlinear skbs */
+	memcpy(desc[q->head].txwi, txwi_ptr, txwi_len);
+	desc[q->head].addr = buf->addr;
+	desc[q->head].ctrl = FIELD_PREP(NPU_TX_DMA_DESC_VEND_LEN_MASK, txwi_len) |
+			     FIELD_PREP(NPU_TX_DMA_DESC_LEN_MASK, skb->len) |
+			     NPU_TX_DMA_DESC_DONE_MASK;
+
+	ret = q->head;
+	q->entry[q->head].skip_buf0 = true;
+	q->entry[q->head].skip_buf1 = true;
+	q->entry[q->head].txwi = NULL;
+	q->entry[q->head].skb = NULL;
+	q->entry[q->head].wcid = 0xffff;
+
+	q->head = (q->head + 1) % q->ndesc;
+	q->queued++;
+
+	return ret;
+}
+
+void mt76_npu_txdesc_cleanup(struct mt76_queue *q, int index)
+{
+	struct airoha_npu_tx_dma_desc *desc = q->desc;
+
+	if (!mt76_queue_is_npu_tx(q))
+		return;
+
+	desc[index].ctrl &= ~NPU_TX_DMA_DESC_DONE_MASK;
+}
+
+void mt76_npu_queue_setup(struct mt76_dev *dev, struct mt76_queue *q)
+{
+	int qid = FIELD_GET(MT_QFLAG_WED_RING, q->flags);
+	bool xmit = mt76_queue_is_npu_tx(q);
+	struct airoha_npu *npu;
+
+	if (!mt76_queue_is_npu(q))
+		return;
+
+	rcu_read_lock();
+	npu = rcu_dereference(dev->mmio.npu);
+	if (npu)
+		q->wed_regs = npu->ops.wlan_get_queue_addr(npu, qid, xmit);
+	rcu_read_unlock();
+}
+
+int mt76_npu_init_rxd(struct mt76_dev *dev, struct airoha_npu *npu)
+{
+	u32 val;
+
+	if (npu->ops.wlan_get_rx_desc_base(npu, 0, &val)) {
+		dev_warn(dev->dev,
+			 "failed retriving npu wlan rx ring0 addr\n");
+		return -ENOMEM;
+	}
+	writel(val, &dev->q_rx[MT_RXQ_RRO_BAND0].regs->desc_base);
+
+	if (npu->ops.wlan_get_rx_desc_base(npu, 1, &val)) {
+		dev_warn(dev->dev,
+			 "failed retriving npu wlan rx ring1 addr\n");
+		return -ENOMEM;
+	}
+	writel(val, &dev->q_rx[MT_RXQ_RRO_BAND1].regs->desc_base);
+
+	if (npu->ops.wlan_get_rx_desc_base(npu, 9, &val)) {
+		dev_warn(dev->dev,
+			 "failed retriving npu wlan rxdmad_c ring addr\n");
+		return -ENOMEM;
+	}
+	writel(val, &dev->q_rx[MT_RXQ_RRO_RXDMAD_C].regs->desc_base);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mt76_npu_init_rxd);
+
+int mt76_npu_init_txd(struct mt76_dev *dev, struct airoha_npu *npu, int size)
+{
+	dma_addr_t dma_addr;
+	void *p;
+	u32 val;
+	int i;
+
+	for (i = MT_BAND0; i < MT_BAND2; i++) {
+		if (npu->ops.wlan_get_rx_desc_base(npu, i + 5, &val)) {
+			dev_warn(dev->dev,
+				 "failed retriving npu wlan tx ring addr\n");
+			continue;
+		}
+		writel(val, &dev->phys[i]->q_tx[0]->regs->desc_base);
+
+		p = dmam_alloc_coherent(dev->dma_dev, size * 256, &dma_addr,
+					GFP_KERNEL);
+		if (!p)
+			return -ENOMEM;
+
+		if (npu->ops.wlan_set_tx_buf_space_base(npu, i, dma_addr))
+			dev_warn(dev->dev,
+				 "failed setting npu wlan queue buf addr\n");
+
+		p = dmam_alloc_coherent(dev->dma_dev, size * 256, &dma_addr,
+					GFP_KERNEL);
+		if (!p)
+			return -ENOMEM;
+
+		if (npu->ops.wlan_set_tx_buf_space_base(npu, i + 5, dma_addr))
+			dev_warn(dev->dev,
+				 "failed setting npu wlan tx buf addr\n");
+
+		p = dmam_alloc_coherent(dev->dma_dev, 1024 * 256, &dma_addr,
+					GFP_KERNEL);
+		if (!p)
+			return -ENOMEM;
+
+		if (npu->ops.wlan_set_tx_buf_space_base(npu, i + 10,
+							dma_addr))
+			dev_warn(dev->dev,
+				 "failed setting npu wlan tx buf base\n");
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mt76_npu_init_txd);
+
+int mt76_npu_rx_queue_init(struct mt76_dev *dev, struct mt76_queue *q)
+{
+	int err, irq, qid = q - &dev->q_rx[0];
+	int size, index = qid - MT_RXQ_NPU0;
+	struct airoha_npu *npu;
+	const char *name;
+
+	rcu_read_lock();
+
+	npu = rcu_dereference(dev->mmio.npu);
+	irq = npu && index < ARRAY_SIZE(npu->irqs) ? npu->irqs[index]
+						   : -EINVAL;
+	rcu_read_unlock();
+
+	if (irq < 0)
+		return irq;
+
+	mutex_lock(&dev->mutex);
+
+	q->flags = MT_NPU_Q_RX(index);
+	size = qid == MT_RXQ_NPU1 ? NPU_RX1_DESC_NUM : NPU_RX0_DESC_NUM;
+	err = dev->queue_ops->alloc(dev, q, 0, size,
+				    MT76_NPU_RX_BUF_SIZE, 0);
+	if (err)
+		goto unlock;
+
+	name = devm_kasprintf(dev->dev, GFP_KERNEL, "mt76-npu" ".%d", index);
+	if (!name) {
+		err = -ENOMEM;
+		goto unlock;
+	}
+
+	err = devm_request_irq(dev->dev, irq, mt76_npu_irq_handler,
+			       IRQF_SHARED, name, q);
+	if (err)
+		goto unlock;
+
+	netif_napi_add(&dev->napi_dev, &dev->napi[qid], mt76_npu_rx_poll);
+	mt76_npu_fill_rx_queue(dev, q);
+	napi_enable(&dev->napi[qid]);
+unlock:
+	mutex_unlock(&dev->mutex);
+
+	return err;
+}
+EXPORT_SYMBOL_GPL(mt76_npu_rx_queue_init);
+
+void mt76_npu_init_tx_done(struct mt76_dev *dev, struct airoha_npu *npu)
+{
+	if (npu->ops.wlan_set_txrx_reg_addr(npu, 2, 0, 0, 0, 0))
+		dev_warn(dev->dev, "failed setting npu wlan txrx addr2\n");
+
+	if (npu->ops.wlan_set_txrx_reg_addr(npu, 7, 0, 0, 0, 0))
+		dev_warn(dev->dev, "failed setting npu wlan txrx addr7\n");
+}
+EXPORT_SYMBOL_GPL(mt76_npu_init_tx_done);
+
+static int mt76_npu_setup_tc_block_cb(enum tc_setup_type type,
+				      void *type_data, void *cb_priv)
+{
+	struct mt76_phy *phy = cb_priv;
+	struct airoha_ppe_dev *ppe_dev;
+	int err = ENODEV;
+
+	if (type != TC_SETUP_CLSFLOWER)
+		return -EOPNOTSUPP;
+
+	rcu_read_lock();
+	ppe_dev = rcu_dereference(phy->dev->mmio.ppe_dev);
+	if (ppe_dev)
+		err = ppe_dev->ops.setup_tc_block_cb(ppe_dev, type_data);
+	rcu_read_unlock();
+
+        return err;
+}
+
+static int mt76_npu_setup_tc_block(struct mt76_phy *phy,
+				   struct net_device *dev,
+				   struct flow_block_offload *f)
+{
+	flow_setup_cb_t *cb = mt76_npu_setup_tc_block_cb;
+	static LIST_HEAD(block_cb_list);
+	struct flow_block_cb *block_cb;
+
+	if (f->binder_type != FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS)
+		return -EOPNOTSUPP;
+
+	if (!tc_can_offload(dev))
+		return -EOPNOTSUPP;
+
+	f->driver_block_list = &block_cb_list;
+	switch (f->command) {
+	case FLOW_BLOCK_BIND:
+		block_cb = flow_block_cb_lookup(f->block, cb, dev);
+		if (block_cb) {
+			flow_block_cb_incref(block_cb);
+			return 0;
+		}
+
+		block_cb = flow_block_cb_alloc(cb, dev, phy, NULL);
+		if (IS_ERR(block_cb))
+			return PTR_ERR(block_cb);
+
+		flow_block_cb_incref(block_cb);
+		flow_block_cb_add(block_cb, f);
+		list_add_tail(&block_cb->driver_list, &block_cb_list);
+		return 0;
+	case FLOW_BLOCK_UNBIND:
+		block_cb = flow_block_cb_lookup(f->block, cb, dev);
+		if (!block_cb)
+			return -ENOENT;
+
+		if (!flow_block_cb_decref(block_cb)) {
+			flow_block_cb_remove(block_cb, f);
+			list_del(&block_cb->driver_list);
+		}
+		return 0;
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+int mt76_npu_net_setup_tc(struct ieee80211_hw *hw, struct ieee80211_vif *vif,
+			  struct net_device *dev, enum tc_setup_type type,
+			  void *type_data)
+{
+	struct mt76_phy *phy = hw->priv;
+
+	if (!tc_can_offload(dev))
+		return -EOPNOTSUPP;
+
+	if (!mt76_npu_device_active(phy->dev))
+		return -EOPNOTSUPP;
+
+	switch (type) {
+	case TC_SETUP_BLOCK:
+	case TC_SETUP_FT:
+		return mt76_npu_setup_tc_block(phy, dev, type_data);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+EXPORT_SYMBOL_GPL(mt76_npu_net_setup_tc);
+
+int mt76_npu_init(struct mt76_dev *dev)
+{
+	struct airoha_ppe_dev *ppe_dev;
+	struct airoha_npu *npu;
+	int err = 0;
+
+	mutex_lock(&dev->mutex);
+
+	npu = airoha_npu_get(dev->dev, NULL);
+	if (IS_ERR(npu)) {
+		request_module("airoha-npu");
+		npu = airoha_npu_get(dev->dev, NULL);
+	}
+
+	if (IS_ERR(npu)) {
+		err = PTR_ERR(npu);
+		goto unlock;
+	}
+
+	ppe_dev = airoha_ppe_get_dev(dev->dev);
+	if (IS_ERR(ppe_dev)) {
+		request_module("airoha-eth");
+		ppe_dev = airoha_ppe_get_dev(dev->dev);
+	}
+
+	if (IS_ERR(ppe_dev)) {
+		err = PTR_ERR(ppe_dev);
+		goto unlock;
+	}
+
+	dev->dma_dev = npu->dev;
+	rcu_assign_pointer(dev->mmio.npu, npu);
+	rcu_assign_pointer(dev->mmio.ppe_dev, ppe_dev);
+	synchronize_rcu();
+unlock:
+	mutex_unlock(&dev->mutex);
+
+	return err;
+
+}
+EXPORT_SYMBOL_GPL(mt76_npu_init);
+
+void mt76_npu_deinit(struct mt76_dev *dev)
+{
+	struct airoha_ppe_dev *ppe_dev;
+	struct airoha_npu *npu;
+
+	mutex_lock(&dev->mutex);
+
+	npu = rcu_replace_pointer(dev->mmio.npu, NULL,
+				  lockdep_is_held(&dev->mutex));
+	if (npu)
+		airoha_npu_put(npu);
+
+	ppe_dev = rcu_replace_pointer(dev->mmio.ppe_dev, NULL,
+				      lockdep_is_held(&dev->mutex));
+	if (ppe_dev)
+		airoha_ppe_put_dev(ppe_dev);
+
+	mutex_unlock(&dev->mutex);
+
+	mt76_npu_queue_cleanup(dev, &dev->q_rx[MT_RXQ_NPU0]);
+	mt76_npu_queue_cleanup(dev, &dev->q_rx[MT_RXQ_NPU1]);
+}
+EXPORT_SYMBOL_GPL(mt76_npu_deinit);
--- a/tx.c
+++ b/tx.c
@@ -859,23 +859,20 @@ void __mt76_set_tx_blocked(struct mt76_d
 }
 EXPORT_SYMBOL_GPL(__mt76_set_tx_blocked);
 
-int mt76_token_consume(struct mt76_dev *dev, struct mt76_txwi_cache **ptxwi,
-		       u8 phy_idx)
+int __mt76_token_consume(struct mt76_dev *dev, struct mt76_txwi_cache **ptxwi,
+			 u8 phy_idx, int token_start)
 {
-	int token = -EINVAL, start = 0;
 	struct mt76_phy *phy = mt76_dev_phy(dev, phy_idx);
-	#ifdef CONFIG_NET_MEDIATEK_SOC_WED	
-	if (mtk_wed_device_active(&dev->mmio.wed))
-		start = dev->mmio.wed.wlan.nbuf;
-	#endif
+	int token = -EINVAL;
+
 	spin_lock_bh(&dev->token_lock);
 
 	if (phy->tokens >= dev->token_threshold)
 		goto out;
 
-	token = idr_alloc(&dev->token, *ptxwi, start, start + dev->token_size,
-			  GFP_ATOMIC);
-	if (token >= start) {
+	token = idr_alloc(&dev->token, *ptxwi, token_start,
+			  token_start + dev->token_size, GFP_ATOMIC);
+	if (token >= token_start) {
 		dev->token_count++;
 
 		(*ptxwi)->phy_idx = phy_idx;
@@ -896,7 +893,7 @@ out:
 
 	return token;
 }
-EXPORT_SYMBOL_GPL(mt76_token_consume);
+EXPORT_SYMBOL_GPL(__mt76_token_consume);
 
 int mt76_rx_token_consume(struct mt76_dev *dev, void *ptr,
 			  struct mt76_rxwi_cache *r, dma_addr_t phys)
